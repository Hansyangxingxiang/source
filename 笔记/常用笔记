hansyang@yunify.com 
QkKkPjNV0P 

OA：https://track.yunify.com/secure/RapidBoard.jspa?rapidView=73&quickFilter=166&quickFilter=150
https://cwiki.yunify.com/login.action?os_destination=%2Findex.action&permissionViolation=true
yangxx/Yangxing@1234

https://yunify.anybox.qingcloud.com/folders/13146
https://yunify.anybox.qingcloud.com/folders/742099
这个是安装部署文档

mysql -uneonsan -pzhu88jie -h192.168.0.2  "source /etc/neonsan/initdb.sql"

手动搭建neonsan环境遇到问题记录：
1.ubuntu启动zookeeper报错：Syntax error: "(" unexpected (expecting "fi")
原因：ubuntu_16.04.5 /bin/sh软连接到dash，有可能是对应的zookeeper的版本支持ubuntu_16.04.5不太好
解决办法：把/bin/sh 软连接到bash。

2.安装galera数据库时按照指导文档配置好源后，无法安装成功，报源中找不到对应的安装？
解决办法：用dpkg -i 安装，这个比较麻烦，无法解决安装包依赖问题，需要把依赖的包一个一个的安装，比较麻烦。

3.安装neonsan后，发现center、stor、monitor一直在重启？并且收到内存过高的告警？
原因：刚开始环境内存不够，把stor的参数改小，依然在不断的重启，后面通过排查发现center、monitor配置中配置的mysql数据库密码没改对。
通过搭建环境对neonsan的理解：neonsan组件包含：zk集群、Mysql数据库集群、center控制集群、stor存储节点、monitor等组成。
zk集群：统一管理控制节点和存储节点；
Mysql数据库：存储集群相关元数据；
center控制集群：卷创建、删除等的控制；
stor存储节点：负责数据io请求：
monitor：监控模块。


monitor模块学习：：
1.由neonmonitor, prometheus, alertmanager组成，prometheus, alertmanager为业界开源的组件，实际neonmonitor是实现Prometheus的Exporters，把metrics暴露给prometheus；
2.monitor阅读代码理解，监控分三个大的部分：
（1）monitorStore： store iops，store tcp network，store rdma network，Cas Core Write，io usage，port status
（2）monitorStatus：store status，store ssd，volume status，center role，mysql-plus status，zookeeper status
（3）本节点：node cpu percen，node memory
3.monitor获取数据的方式：调用neonsan的api 接口获取；在本节点执行go cmd 或调用go的工具包获取。

NeonSAN 编译学习：访问 VCenter 控制台虚拟机创建；

1.创建卷报错：INFO[0000] create volume failed. Reason:HTTP status:500  rc:-1 reason:Failed to choose volume place: resource group nodes not enough for create volume
原因：创建卷需要创建资源组；

2.创建资源组时报：FATA[0000] Failed to add resource group node, reason:HTTP status:200  rc:-507 reason:store id 2 not exist.
原因：搭建环境时有两个节点的stor id忘记改，并且重复了，重新修改配置，重启服务，重启不不生效，需要手动更改数据库。
为什么重启不能生效，需要清除数据库？
重新清除数据库：drop database neonsan;source /etc/neonsan/initdb.sql;各个节点centor,store
为什么kill -9 store，store重启，centor会重启？

创建一个3副本的卷，其中有两个节点重启，导致2个副本error，store重启后为什么不能自动恢复？需要neonsan set_parameter --parameter auto_recovery --value 1
为啥不能开关默认开启？

3.日志文件中无法看到打印日志的是在那个文件的哪一行？

副本粒度64G


下载代码：
git clone -b qa https://git.internal.yunify.com/SAN2.0/qfa.git
git clone -b qa https://git.internal.yunify.com/SAN2.0/qfcenter.git
git clone -b qa https://git.internal.yunify.com/SAN2.0/monitor.git
cd qfa/
git submodule init 
git submodule update


创建Job流程：
1."local_access"的任务，如果任务属于WAITING，PENDING，如果卷名相同，需要把已经存在的任务删除；
2.如果存在完全相同未完成的任务，则报错;
3.如果METRO_SYNC，且任务已经完成或者失败，则需要把任务删除；
4.如果是waiting jobs，waitingJobs不能大于等于队列queue.Capacity的一半；
5.waitingJobs + pendingJobs的任务不能大于等于queue.Capacity；
6.waitingJobs + pendingJobs的任务不能大于等于queue.Capacity；
7.创建任务；
8.如果不是waiting的任务，则把waiting的任务变成pending的任务；

删除任务：
1.带jobId：job是pending或者processing的任务不能删除；从zk中删除，然后从队列中删除；
2.删除一段时间期间的job:循环遍历任务，只删除在时间期间内不是PROCESSING、PENDING的任务；

任务执行：
启动：
MqInit
1.创建集群队列节点，初始化队列MAP；
2.初始化队列消费函数；
3.从队列中获取所有队列名；
4.在个队列节点下创建WAITING, PENDING, PROCESSING, COMPLETED, FAILED的子节点；
5.把PROCESSING的job已到FAILED；
6.设置队列capacity
7.内存中保存queueInfo，并且把zk的job信息保存到内存；
8.zk中创建各个队列；
9.job任务执行;executeTimingJobs 定时把waiting的任务移到pending；doExecute：把pending的job移到processing，并执行任务函数；
10.clearOverdueJob清除超时任务。

统信UOS和华为泰山ARM平台的NeonSan的编译打包：
1.实际就是在新的架构平台上实现NeonSan的编译打包，由于新的架构平台NeonSan编译依赖的lib库没有现存的（比如zk、toml），在编译NeonSan之前需要用源码编译对应的lib库；
2.libtoml：一种可以解析配置文件的库，有点类似于解析json格式的配置文件的模块库；、
3.在新的平台中编译可能会编译错误，需要修改适配对应的源码；


本周工作：
	1.完成NEONSAN-623：neonsan add_ssd超时问题；
	2.修改NEONSAN-634：升级脚本检查qingstor neonsan-mysql状态检查代码编写，过程中梳理升级脚本流程并输出文档流程，文档如附件，已经进行脚本的单函数测试，整套升级脚本流程待搭环境测试，整体完成 %80；
	3.完成ubuntu18.04.01 arm OS的neonsan的编译打包；
	4.学习nesonmonitor代码，prometheus框架；nesonmonitor实际为prometheus的一个exporter组件，nesonmonitor 利用Prometheus client API注册收集指标；在本地完成Prometheus，node_exporter、graph的环境搭建；学习node_exporter已经有哪些资源监控信息；
	

下周计划：
	1.解决NEONSAN-34：正在recovery的卷无法停止；
	2.搭建环境验证升级流程；
	3.学习版本发布流程和代码学习；
	4.ubuntu18.04.01 arm OS的neonsan的编译打包流程脚本自动化；




升级流程：
1.基本工具检查：basic_tools_check
（1）检查当前节点是否安装jq 、curl、nohup工具；
（2）集群的所有节点必须安装prlimit、ping、nc工具；
2.检查集群状态：check_cluster_status
（1）检查store状态；check_store_status curl发rest消息获取所有store的IP，curl发rest消息回去获取store的状态，检查是否OK；     问题：获取store IP是从数据库中获取，还需要执行neonsan list_store的目的？
（2）检查ssd状态： check_ssd_status
（3）检查port状态：check_port_status，获取所有port端口的IP，检查每个IP端口的状态；
（4）检查各服务状态：check_supervisor_service_status，ssh到每个节点执行supervisor命令查询；supervisorctl status；neoncenter neonstore neonmonitor zookeeper，检查数据库的状态
（5）检查check_started_live_nodes
（6）检查卷的状态：check_volume_status
（7）检查job状态；pending|processing|waiting状态的job退出
3.设置升级的原版本和目标版本：set_version；
4.拷贝升级到目标节点：copy_upgrade_packet；
5.升级前准备：preparation_before_upgrade：备份数据库，查询卷、ssd、store、port、parameter信息，重定向到本地文件；把auto_balance、auto_recovery开关关闭；
6.升级到center：upgrade_center_service：先升级leader，检查集群状态，检查内容和步骤2一样，备份配置center配置文件，覆盖安装软件包，删除supervisor的center服务，重新reload，检查升级后的版本，等待center被拉起，检查center的状态    问题：fellower的所有节点为啥不能批量升级？
7.检查升级后的版本和目标版本是否一样：check_db_ver
8.升级tool：upgrade_tool_service：获取集群的节点，一个一个节点的升级，备份qbd.conf配置，移除too的软件包，然后安装，检查升级的版本是否正确；
9.升级store：upgrade_store_service：升级前状态检查，检查是否存在ERROR的卷，存在错误或者降级的卷，则设置自动恢复卷的功能，检查集群的状态，执行升级store的操作：检查资源组：没有资源组会创建资源组，进入维护模式，备份配置，覆盖安装软件包，等到store状态OK，退出维护模式，检查升级后的版本是否符合预期，升级后检查各卷的状态，进程服务状态；
10.升级monitor：upgrade_monitor_service：备份配置，覆盖安装软件包，配置没有，则添加mysql_cluster，这个分成galer数据库和xenondb，修改timeInterval的值，添加local_storeip，重启monitor服务，检查版本
11.升级数据服务：upgrade_galera_cluster_service：检查数据状态，备份配置，覆盖安装，恢复配置，重启服务，检查版本，检查状态；
12.升级后的操作，查询资源组、卷、ssd等信息，升级完成后的清理；
13.结束；
14.退出维护模式；

wget https://studygolang.com/dl/golang/go1.12.5.linux-arm64.tar.gz
tar -zxvf go1.12.5.linux-arm64.tar.gz  -C /home/yangxx/
tar -zxvf gopath1.12.5.tar.gz -C /home/yangxx/
export GOROOT=/home/yangxx/go
export GOPATH=/home/yangxx/gopath
export PATH=$GOROOT/bin:$PATH
git clone -b v2.5.0-release https://git.internal.yunify.com/SAN2.0/qfa.git
git checkout -b v2.5.0-release
git clone -b v2.5.0-release https://git.internal.yunify.com/SAN2.0/qfcenter.git
git clone -b v2.5.0-release https://git.internal.yunify.com/SAN2.0/monitor.git
git clone https://git.internal.yunify.com/SAN2.0/libtoml.git

yangxx@ubuntu18041-compile:~/libtoml$ make
make[2]: *** No rule to make target 'RAGEL-NOTFOUND', needed by 'toml_parse.c'.  Stop.
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/toml.dir/all' failed
make[1]: *** [CMakeFiles/toml.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 

apt install ragel

[ 80%] Building C object CMakeFiles/toml.dir/toml_parse.c.o
toml_parse.rl:13:10: fatal error: unicode/ustring.h: No such file or directory
 #include <unicode/ustring.h>
          ^~~~~~~~~~~~~~~~~~~
compilation terminated.

改动有这些文件：
modified: store/src/core/store_node.go
modified: store/src/main.go
modified: store/src/qfa_md5.cpp
modified: thirdparty/zookeeper/src/c/Makefile.am
modified: thirdparty/zookeeper/src/c/acinclude.m4
modified thirdparty/zookeeper/src/c/configure.ac

apt-get install libicu-dev
apt-get install uuid-dev
apt-get install libaio-dev


NEONSAN-653	
[云平台易捷版]galera 集群在断电/恢复电力之后不能自动恢复
1.1.2版本自愈流程：
1.bootstrap有1，就是首节点:（bootstrap是最后关机，数据是最新）
2.如果找不到bootstrap为1的节点，就找seqNO最大的节点当首节点；（seqNO大数据最新）
3.如果seqNO都相同，那就随便找一个当做首节点，目前找IP地址大的当首节点。

现在模拟断电/恢复：三个节点同时kill -9 mysqld进程多次，发现无法自愈：
不能自愈原因：有一个节点获取seqNO失败，无法选出首节点

目前运维经验中遇到无法获取的seqno的解决方法是：获取seqno命令中加--user=mysql
所以当前要做的是：
1.看能否从以前遇到问题的局点中去到日志，或者后面遇到这种问题收集日志分析，确认不能自愈也是获取的seqno命令缺少--user=mysql；
2.如果是，自愈流程获取seqno的命令中也加上--user=mysql。
3.当前遇到加--user=mysql也无法获取seqno，网上解决方法是直接删除ib_logfile0，ib_logfile1，分析这种操作的风险，为后面运维积累经验，或者如果没有风险，这个步骤也做到自愈流程中。


 NEONSAN-34 【recovery volume】正在recovery的卷无法停止：1.删除一个正在恢复的卷，提示正在恢复；2.增加停止recovery的CLI
 
 neonfio -name=case -ioengine=qbd -direct=1 -bs=4k -volume=pool_name/yangxx4  -config_file=/etc/neonsan/qbd.conf -rw=randwrite -iodepth=10 -time_based -numjobs=1 -runtime=500 -use_tcp=1

 本周工作：
	1.完成NEONSAN-653[云平台易捷版]galera 集群在断电/恢复电力之后不能自动恢复：修改、验证：已经push，待review；
	2.完成NEONSAN-34 【v2.1.5】【recovery volume】正在recovery的卷无法停止：修改、验证：已经push，待review
	3.完成NEONSAN-665 neonsan stop_job优化，已经合入3.0；
	4.出ubuntu 18.04.1 v2.5.1 arm版本的包。
	
下周计划：
	1.熟悉了解《大规模集群方案》；
	
	
galera_cluster 数据库初始化未同步，一段时间内数据不一致的问题；
https://galeracluster.com/library/documentation/schema-upgrades.html

kill -9 mysql进程，查看集群状态

从10节点登录firstbox 172.31.45.2
172.31.45.10 express1ar01n00
172.31.45.11 express1ar01n01
172.31.45.12 express1ar01n02 

sync;date;echo 1 > /proc/sys/kernel/sysrq && echo b > /proc/sysrq-trigger



export BRANCH_NAME="qa"
export BUILD_PATH=/z0/yangxingxiang/qa
export BUILD_VERSION="debs-ubuntu_16.04.5"
export BUILD_ONLY="true"

当使用新版本qbd时： Store 与client 保持原来的zk 配置不变，升级结束后，老qbd通过qbd.conf 中的配置依然与当前rg 的local center 建立连接， 并且将zk ip list 和local leader center写入到一个本地文件中，下次open_volume时 直接从本地文件中连接zk 获取local leader center ; 若失败则连接上次保存的local leader center。
当使用的qbd版本不支持保存local leader center 和local zk 到本地文件时，老的qbd 每次都从qbd.conf 中获取qfcip 与local center进行连接。
当扩容了一个新rg 之后，老版本的qbd.conf 要访问新rg 上的卷时， 在通过qbd.conf 连接到local center 后， local center 检查卷不属于本资源组， 于是转发到global center , global再转发到对应的rg 进行处理。在这种情况下， 新版本的qbd将只在首次需要进行两次转发， 后续都可以从本地文件获取新的所属rg local center。 老版本的center 则每次都需要转发2次。

https://tenant.quanxiangyun.com/qingcloud/manage/tickets/1598
富春云prometheus的iops信息为0。检查集群状态正常，节点的node-exporter，获取curl 9100/metrics信息正常，prometheus都正常：
原因：统计iops的变量类型（sig_atomic_t）是32位的，运行时间长了可能会越界出现负数，monitor上报prometheus的时候出现这种异常时直接上报为0，并且monitor中直接把lastIoStat清理，没有把本次的结果赋值给lastIoStat，导致一直上报0.
修改代码：把本次的查询结果赋值给lastIoStat，出现负数的时候，属于增量计算的，也能正常吧结果上报到prometheus
临时的解决方法：依次重启rg下面节点的store进程（重启进程影响较大，谨慎操作）

https://boss.qingcloud.com/tickets/tk-mqtsiogds3l#
泉州移动neonsan混插环境，iops只有1.2k左右，在交付前请研发检查。tv1 403 307 099.   w8rn69
分析：之前看错了，iops 12k左右，没有预热的情况下是正常，按照最后一步测试，大致可以测出来30K左右的iops，符合验收标准，需要按照交付文档测试

https://tenant.quanxiangyun.com/qingcloud/manage/tickets/1627
青岛创恒    私有云  sanc 环境  neoncenter 剩一个节点 running，现在启动了三台节点的neoncenter，neonsan命令还是报错
FATA[0002] Failed to list ssd. reason:HTTP status:<nil>, reason:Get http://10.21.0.21:2600/qfa?op=list_ssd&refresh_capacity=false: dial tcp 10.21.0.21:2600: connect: connection refused 
虚拟机无法启动
客户着急，麻烦尽快上来看下

tv 1350 470 643 2cwg57
center的一个历史bug，在切换center时偶尔可能会阻塞，客户用的还是老版本
root@neonsan41:~# neonsan list_version
NeonSAN Cli Version: 2.2.0 build:82399ce-Jan 10 2020 01:21:36
+---------+-------------------------------------+---------------------+
| NEONSAN |            BUILD VERSION            |    UPDATED TIME     |
+---------+-------------------------------------+---------------------+
| center  | 2.2.0,build:359eb7e-20200110.012237 | 2020-01-16 22:25:24 |
| monitor | 2.2.0,build:67e0050-20200110.012237 | 2020-01-16 23:14:28 |
| store   | 2.2.0,build:82399ce-20200110.012237 | 2020-01-16 22:35:47 |
+---------+-------------------------------------+---------------------+

https://track.yunify.com/browse/NEONSAN-163
https://track.yunify.com/browse/NEONSAN-363

https://boss.qingcloud.com/tickets/tk-hukrs78s2qa#
用户虚机挂载的vos盘 目录有段时间无法写入，排查系统日志，有这个报错，这个报错的触发原因需要协助排查一下

问题原因：网络异常引起center切换，当前版本存在一个bug，job队列超了会引起center切换失败，建议升级到新版彻底解决。

https://boss.qingcloud.com/tickets/tk-hukrs78s2qa# 338 137 162  1qaz@WSX   teamviewer
用户虚机挂载的vos盘 目录有段时间无法写入，上周五也有同类问题，期间持续了将近一个小时

root@neonsan41:~# cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-08-25 16:23:42.222] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-08-25 17:08:42.099] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-08-28 17:39:02.951] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 09:55:45.765] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan41:~# ssh 10.21.0.11
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-39-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
New release '18.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Thu Sep 10 13:38:19 2020 from 10.21.0.2
root@neonsan11:~#  cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-04 23:57:00.439] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:44:39.874] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:47:18.261] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:04:57.815] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:18:34.303] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:19:45.681] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:21:46.671] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:26:01.882] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan11:~# ssh 10.21.0.31
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-39-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
New release '18.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Wed Sep  9 09:54:37 2020 from 10.21.0.31
root@neonsan31:~#  cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-05 00:23:34.631] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 09:49:35.607] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan31:~# exit
logout
Connection to 10.21.0.31 closed.
root@neonsan11:~# ll /var/log/* -h^C
root@neonsan11:~# cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-04 23:57:00.439] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:44:39.874] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:47:18.261] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:04:57.815] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:18:34.303] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:19:45.681] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:21:46.671] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:26:01.882] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275


https://boss.qingcloud.com/tickets/tk-fprwho2yprd#
泰康保险 私有云 neonsan服务器重启问题排查
检查过系统日志看不出由于软件导致的重启。需要驻场同事装一下impitool 收集硬件信息


https://boss.qingcloud.com/tickets/tk-3njr9cn4bc4# 上海科创测试neonsan环境，客户反馈在使用iscsi时，有时候会在3点和23点左右连接iscsi中断。检查kern日志io 有delay，继而检查center日志，发现当时在删除snapshot，但是都删除失败了，发现只有19和29这2台删除失败，到19和29检查store日志发现有持续的SLOW IO告警，iostat看磁盘没有到瓶颈，负载也不高，麻烦帮忙看下原因


https://boss.qingcloud.com/tickets/tk-dobjwzaim99
私有云  上海科创
21 22节点neonstore服务异常退出，麻烦看下
tv12        1 421 688 504           isy584

科创这个问题在2.5.2解决了，在store节点连接其他store的时候，用select的方式设置了连接的超时时间，当select监听的套接字fd大于1024时，就会造成函数栈帧内的内存溢出，所以退出函数的时候crash。已经fix掉


本周工作：
1.完成升级脚本jira单NEONSAN-689、NEONSAN-691 修改验证，代码已经merge，升级脚本v1.12已经发布归档；
2.完成NEONSAN-669、NEONSAN-670、NEONSAN671、NEONSAN-672修改，代码已经push，待review；
3.工单处理和跟踪，目前本周有5个工单，4个已经有结果，1个还在分析中；
4.发布v2.5.2的版本；
5.解决Jenkins打包第三方库没有更新到最新节点的问题；

下周计划：
1.大规模集群方案优化中lib common的适配修改；
2.修改jira单；

http://old-releases.ubuntu.com/releases/16.04.5/


cd /root/debs-ubuntu_16.04.5/ && dpkg -r pitrix-neoncenter-dbg pitrix-neonsan-monitor pitrix-neonsan-tool pitrix-neonstore-dbg && dpkg -i pitrix-neoncenter-dbg-3.0.0.0.0.1.deb pitrix-neonsan-monitor-3.0.0.0.0.1.deb  pitrix-neonsan-tool-3.0.0.0.0.1.deb pitrix-neonstore-dbg-3.0.0.0.0.1.deb && supervisorctl stop neoncenter neonmonitor neonstore && supervisorctl start neoncenter neonmonitor neonstore


本周工作：
1.分析青岛创恒 center切换为主之后卡住，引起center切换的原因是center连接不上zk，分析zk异常的原因，可能是存在的bug；https://issues.apache.org/jira/browse/ZOOKEEPER-1582
2.完成NEONSAN-715变量越界，iops显示为0的修改和验证，代码已经merge；
3.完成NEONSAN-701 API list_volume支持互斥组名过滤的修改验证，代码已经push，待review；
4.完成NEONSAN-113 API list_store 支持rg_id rg_name,rg_label过滤；
5.完成NEONSAN-264 list_replica_location 支持更多查询选项，代码已经push，待review；
6.完成lib common中open_volume_common流程中获取qfcip流程适配修改的测试，此修改方法可能对一个节点对接多套neonsan（即一个节点多个qbd.conf）的场景不支持；


下周计划：
1.修改jira单；

ubuntu iso下载地址：
http://old-releases.ubuntu.com/releases/16.04.5/


open_volume_comcom的查询qfcip流程适配修改：
原来流程：通过/etc/neonsan/qbd.conf获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询得到qfcip，
修改成如下：
1. /etc/neonsan/local_zk_qfc.conf存在，且local_zk ip local cluster_name 和qbd.conf相同：则用/etc/neonsan/local_zk_qfc.conf查询，否则用/etc/neonsan/qbd.conf查询
（1）.从 /etc/neonsan/local_zk_qfc.conf 获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询（流程和原先的不变）
（2）.如果（1）查询失败，从/etc/neonsan/qbd.conf查询（查询流程和1一样）
（3）.如果是通过/etc/neonsan/qbd.conf查询成功的，把zk ips和查询到的qfc IP保存到/etc/neonsan/local_zk_qfc.conf中


本周工作：
1.NEONSAN-684分析，3.0.0已经不会在zk 里面写uuid，问题不存在；2.5.x版本中一般remove ssd应该也不会出现zk存在、数据库中不存在的情况，除非从zk中删除的时候异常，没有return，然后又把数据库中的删除，因此解决此问题需要从源头解决，即，从zk删除异常时返回失败，不继续从删数据库删除。
2.open_volume_comcom的查询qfcip流程适配修改和验证，可能和最初的需求理解有偏差，还需进一步分析设计；
3.完成ubun18.04版本包的自动化；
4.neonsan容器化部署研究，初步想了一些镜像打包的思路和方法，下一步还需要参考业界分布式存储容器化的方案、容器team团队给的赋能和经验分享；
5.v3.0.1、v2.5.5的打包工作，并且出手动出v3.0.1使用openssl库的打包；

下周工作：
1.继续neonsan容器化部署研究；
2.open_volume_comcom适配的分析、设计、修改；


本周工作：
1.完成NEONSAN-544修改验证，代码已经push，待review；
2.完成NEONSAN-764修改验证，代码已经push，待review：
3.完成NEONSAN-593修改验证，代码merge
4.完成NEONSAN-801修改验证，代码merge；
5.完成NEONSAN-766修改验证，代码已经push，待review：
6.完成v2.5.5版本发布相关材料准备和归档，v3.0.1打包相关工作。

下周工作：
1.继续neonsan容器化部署研究；
2.open_volume_comcom适配的需求澄清，修改、验证；


open_volume_comcom的查询qfcip流程适配：
背景：后续的大规模部署新架构将会是一个global center 管理多个local center , 每个local center 集群负责它所管理的rg的卷的错误处理和open volume等操作，这次需求的适配为了兼容后续超算项目从3.x 版本升级到新架构的场景。期望当使用新版本qbd时，Store 与client 保持原来的zk 配置不变
原来流程：通过/etc/neonsan/qbd.conf获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询得到qfcip，
修改成如下：
1. /etc/neonsan/local_zk_qfc.conf存在，且local_zk ip local cluster_name 和qbd.conf相同：则用/etc/neonsan/local_zk_qfc.conf查询，否则用/etc/neonsan/qbd.conf查询
（1）.从 /etc/neonsan/local_zk_qfc.conf 获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询（流程和原先的不变）
（2）.如果（1）查询失败，从/etc/neonsan/qbd.conf查询（查询流程和1一样）
（3）.如果是通过/etc/neonsan/qbd.conf查询成功的，把zk ips和查询到的qfc IP保存到/etc/neonsan/local_zk_qfc.conf中




#define MAX_BUFFER_SIZE_128  128
#define MAX_BUFFER_SIZE_1024  1024


struct qfa_options {
    char userkey_name[MAX_BUFFER_SIZE_128];
    char userkey_passwd[MAX_BUFFER_SIZE_128];
    char zk_ips[MAX_BUFFER_SIZE_1024]
    char center_ips[MAX_BUFFER_SIZE_1024];
}

int open_volume_common(struct qfa_client_volume *vol, int lib_ver, struct qfa_options *options);


lib common适配修改：
1.options->zk_ips, options->center_ips没有内容时，open_volume_common按照原流程查询qfcip；
2.options->zk_ips, options->center_ips有内容时，open_volume_common使用options->zk_ips, options->center_ips查询qfcip；
3.查询到qfcip，向leader center发送open_volume（此步骤不涉及改动）；
4.解析open_volume返回的卷的属性，把所属的zk_ips,center_ip保存到options->zk_ips, options->center_ips中；

center修改：
1.OpenVolume API输出中增加参数zk_ips，center_ips；
2.两参数直接从该节点的/etc/neonsan/center.conf中读取zookeeper.ip,center.mngt_ip；

qbd修改：从缓存读取到内容（读取不到赋值空）后调用open_volume_common，调用open_volume_common完后从options->zk_ips, options->center_ips读取内容进行缓存；



2020年10月23日讨论纪要问题：
1.qemu qfa_open_volume的缓存问题；
2.qbd client老版本不支持时，open_volumem每次都需要global转发，即：qbd-> global -> local， local-> global->qbd；
3.center_ips缓存多个，leader center ip放在第一个；













 