hansyang@yunify.com 
QkKkPjNV0P 

OA：https://track.yunify.com/secure/RapidBoard.jspa?rapidView=73&quickFilter=166&quickFilter=150
https://cwiki.yunify.com/login.action?os_destination=%2Findex.action&permissionViolation=true
yangxx/Yangxing@1234

https://yunify.anybox.qingcloud.com/folders/13146
https://yunify.anybox.qingcloud.com/folders/742099
这个是安装部署文档

mysql -uneonsan -pzhu88jie -h192.168.0.2  "source /etc/neonsan/initdb.sql"

手动搭建neonsan环境遇到问题记录：
1.ubuntu启动zookeeper报错：Syntax error: "(" unexpected (expecting "fi")
原因：ubuntu_16.04.5 /bin/sh软连接到dash，有可能是对应的zookeeper的版本支持ubuntu_16.04.5不太好
解决办法：把/bin/sh 软连接到bash。

2.安装galera数据库时按照指导文档配置好源后，无法安装成功，报源中找不到对应的安装？
解决办法：用dpkg -i 安装，这个比较麻烦，无法解决安装包依赖问题，需要把依赖的包一个一个的安装，比较麻烦。

3.安装neonsan后，发现center、stor、monitor一直在重启？并且收到内存过高的告警？
原因：刚开始环境内存不够，把stor的参数改小，依然在不断的重启，后面通过排查发现center、monitor配置中配置的mysql数据库密码没改对。
通过搭建环境对neonsan的理解：neonsan组件包含：zk集群、Mysql数据库集群、center控制集群、stor存储节点、monitor等组成。
zk集群：统一管理控制节点和存储节点；
Mysql数据库：存储集群相关元数据；
center控制集群：卷创建、删除等的控制；
stor存储节点：负责数据io请求：
monitor：监控模块。


monitor模块学习：：
1.由neonmonitor, prometheus, alertmanager组成，prometheus, alertmanager为业界开源的组件，实际neonmonitor是实现Prometheus的Exporters，把metrics暴露给prometheus；
2.monitor阅读代码理解，监控分三个大的部分：
（1）monitorStore： store iops，store tcp network，store rdma network，Cas Core Write，io usage，port status
（2）monitorStatus：store status，store ssd，volume status，center role，mysql-plus status，zookeeper status
（3）本节点：node cpu percen，node memory
3.monitor获取数据的方式：调用neonsan的api 接口获取；在本节点执行go cmd 或调用go的工具包获取。

NeonSAN 编译学习：访问 VCenter 控制台虚拟机创建；

1.创建卷报错：INFO[0000] create volume failed. Reason:HTTP status:500  rc:-1 reason:Failed to choose volume place: resource group nodes not enough for create volume
原因：创建卷需要创建资源组；

2.创建资源组时报：FATA[0000] Failed to add resource group node, reason:HTTP status:200  rc:-507 reason:store id 2 not exist.
原因：搭建环境时有两个节点的stor id忘记改，并且重复了，重新修改配置，重启服务，重启不不生效，需要手动更改数据库。
为什么重启不能生效，需要清除数据库？
重新清除数据库：drop database neonsan;source /etc/neonsan/initdb.sql;各个节点centor,store
为什么kill -9 store，store重启，centor会重启？

创建一个3副本的卷，其中有两个节点重启，导致2个副本error，store重启后为什么不能自动恢复？需要neonsan set_parameter --parameter auto_recovery --value 1
为啥不能开关默认开启？

3.日志文件中无法看到打印日志的是在那个文件的哪一行？

副本粒度64G


下载代码：
git clone -b neonio https://git.internal.yunify.com/SAN2.0/qfa.git
git clone -b neonio  https://git.internal.yunify.com/SAN2.0/qfcenter.git
git clone -b neonio  https://git.internal.yunify.com/SAN2.0/monitor.git
cd qfa/
git submodule init 
git submodule update


创建Job流程：
1."local_access"的任务，如果任务属于WAITING，PENDING，如果卷名相同，需要把已经存在的任务删除；
2.如果存在完全相同未完成的任务，则报错;
3.如果METRO_SYNC，且任务已经完成或者失败，则需要把任务删除；
4.如果是waiting jobs，waitingJobs不能大于等于队列queue.Capacity的一半；
5.waitingJobs + pendingJobs的任务不能大于等于queue.Capacity；
6.waitingJobs + pendingJobs的任务不能大于等于queue.Capacity；
7.创建任务；
8.如果不是waiting的任务，则把waiting的任务变成pending的任务；

删除任务：
1.带jobId：job是pending或者processing的任务不能删除；从zk中删除，然后从队列中删除；
2.删除一段时间期间的job:循环遍历任务，只删除在时间期间内不是PROCESSING、PENDING的任务；

任务执行：
启动：
MqInit
1.创建集群队列节点，初始化队列MAP；
2.初始化队列消费函数；
3.从队列中获取所有队列名；
4.在个队列节点下创建WAITING, PENDING, PROCESSING, COMPLETED, FAILED的子节点；
5.把PROCESSING的job已到FAILED；
6.设置队列capacity
7.内存中保存queueInfo，并且把zk的job信息保存到内存；
8.zk中创建各个队列；
9.job任务执行;executeTimingJobs 定时把waiting的任务移到pending；doExecute：把pending的job移到processing，并执行任务函数；
10.clearOverdueJob清除超时任务。

统信UOS和华为泰山ARM平台的NeonSan的编译打包：
1.实际就是在新的架构平台上实现NeonSan的编译打包，由于新的架构平台NeonSan编译依赖的lib库没有现存的（比如zk、toml），在编译NeonSan之前需要用源码编译对应的lib库；
2.libtoml：一种可以解析配置文件的库，有点类似于解析json格式的配置文件的模块库；、
3.在新的平台中编译可能会编译错误，需要修改适配对应的源码；


本周工作：
	1.完成NEONSAN-623：neonsan add_ssd超时问题；
	2.修改NEONSAN-634：升级脚本检查qingstor neonsan-mysql状态检查代码编写，过程中梳理升级脚本流程并输出文档流程，文档如附件，已经进行脚本的单函数测试，整套升级脚本流程待搭环境测试，整体完成 %80；
	3.完成ubuntu18.04.01 arm OS的neonsan的编译打包；
	4.学习nesonmonitor代码，prometheus框架；nesonmonitor实际为prometheus的一个exporter组件，nesonmonitor 利用Prometheus client API注册收集指标；在本地完成Prometheus，node_exporter、graph的环境搭建；学习node_exporter已经有哪些资源监控信息；
	

下周计划：
	1.解决NEONSAN-34：正在recovery的卷无法停止；
	2.搭建环境验证升级流程；
	3.学习版本发布流程和代码学习；
	4.ubuntu18.04.01 arm OS的neonsan的编译打包流程脚本自动化；




升级流程：
1.基本工具检查：basic_tools_check
（1）检查当前节点是否安装jq 、curl、nohup工具；
（2）集群的所有节点必须安装prlimit、ping、nc工具；
2.检查集群状态：check_cluster_status
（1）检查store状态；check_store_status curl发rest消息获取所有store的IP，curl发rest消息回去获取store的状态，检查是否OK；     问题：获取store IP是从数据库中获取，还需要执行neonsan list_store的目的？
（2）检查ssd状态： check_ssd_status
（3）检查port状态：check_port_status，获取所有port端口的IP，检查每个IP端口的状态；
（4）检查各服务状态：check_supervisor_service_status，ssh到每个节点执行supervisor命令查询；supervisorctl status；neoncenter neonstore neonmonitor zookeeper，检查数据库的状态
（5）检查check_started_live_nodes
（6）检查卷的状态：check_volume_status
（7）检查job状态；pending|processing|waiting状态的job退出
3.设置升级的原版本和目标版本：set_version；
4.拷贝升级到目标节点：copy_upgrade_packet；
5.升级前准备：preparation_before_upgrade：备份数据库，查询卷、ssd、store、port、parameter信息，重定向到本地文件；把auto_balance、auto_recovery开关关闭；
6.升级到center：upgrade_center_service：先升级leader，检查集群状态，检查内容和步骤2一样，备份配置center配置文件，覆盖安装软件包，删除supervisor的center服务，重新reload，检查升级后的版本，等待center被拉起，检查center的状态    问题：fellower的所有节点为啥不能批量升级？
7.检查升级后的版本和目标版本是否一样：check_db_ver
8.升级tool：upgrade_tool_service：获取集群的节点，一个一个节点的升级，备份qbd.conf配置，移除too的软件包，然后安装，检查升级的版本是否正确；
9.升级store：upgrade_store_service：升级前状态检查，检查是否存在ERROR的卷，存在错误或者降级的卷，则设置自动恢复卷的功能，检查集群的状态，执行升级store的操作：检查资源组：没有资源组会创建资源组，进入维护模式，备份配置，覆盖安装软件包，等到store状态OK，退出维护模式，检查升级后的版本是否符合预期，升级后检查各卷的状态，进程服务状态；
10.升级monitor：upgrade_monitor_service：备份配置，覆盖安装软件包，配置没有，则添加mysql_cluster，这个分成galer数据库和xenondb，修改timeInterval的值，添加local_storeip，重启monitor服务，检查版本
11.升级数据服务：upgrade_galera_cluster_service：检查数据状态，备份配置，覆盖安装，恢复配置，重启服务，检查版本，检查状态；
12.升级后的操作，查询资源组、卷、ssd等信息，升级完成后的清理；
13.结束；
14.退出维护模式；

wget https://studygolang.com/dl/golang/go1.12.5.linux-arm64.tar.gz
tar -zxvf go1.12.5.linux-arm64.tar.gz  -C /home/yangxx/
tar -zxvf gopath1.12.5.tar.gz -C /home/yangxx/
export GOROOT=/home/yangxx/go
export GOPATH=/home/yangxx/gopath
export PATH=$GOROOT/bin:$PATH
git clone -b 1-dumpzk-getqfcip https://git.internal.yunify.com/SAN2.0/qfa.git
git checkout -b v2.5.0-release
git clone -b 1-dumpzk-getqfcip https://git.internal.yunify.com/SAN2.0/qfcenter.git
git clone -b v2.5.0-release https://git.internal.yunify.com/SAN2.0/monitor.git
git clone https://git.internal.yunify.com/SAN2.0/libtoml.git

yangxx@ubuntu18041-compile:~/libtoml$ make
make[2]: *** No rule to make target 'RAGEL-NOTFOUND', needed by 'toml_parse.c'.  Stop.
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/toml.dir/all' failed
make[1]: *** [CMakeFiles/toml.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 

apt install ragel

[ 80%] Building C object CMakeFiles/toml.dir/toml_parse.c.o
toml_parse.rl:13:10: fatal error: unicode/ustring.h: No such file or directory
 #include <unicode/ustring.h>
          ^~~~~~~~~~~~~~~~~~~
compilation terminated.

改动有这些文件：
modified: store/src/core/store_node.go
modified: store/src/main.go
modified: store/src/qfa_md5.cpp
modified: thirdparty/zookeeper/src/c/Makefile.am
modified: thirdparty/zookeeper/src/c/acinclude.m4
modified thirdparty/zookeeper/src/c/configure.ac

apt-get install libicu-dev
apt-get install uuid-dev
apt-get install libaio-dev


NEONSAN-653	
[云平台易捷版]galera 集群在断电/恢复电力之后不能自动恢复
1.1.2版本自愈流程：
1.bootstrap有1，就是首节点:（bootstrap是最后关机，数据是最新）
2.如果找不到bootstrap为1的节点，就找seqNO最大的节点当首节点；（seqNO大数据最新）
3.如果seqNO都相同，那就随便找一个当做首节点，目前找IP地址大的当首节点。

现在模拟断电/恢复：三个节点同时kill -9 mysqld进程多次，发现无法自愈：
不能自愈原因：有一个节点获取seqNO失败，无法选出首节点

目前运维经验中遇到无法获取的seqno的解决方法是：获取seqno命令中加--user=mysql
所以当前要做的是：
1.看能否从以前遇到问题的局点中去到日志，或者后面遇到这种问题收集日志分析，确认不能自愈也是获取的seqno命令缺少--user=mysql；
2.如果是，自愈流程获取seqno的命令中也加上--user=mysql。
3.当前遇到加--user=mysql也无法获取seqno，网上解决方法是直接删除ib_logfile0，ib_logfile1，分析这种操作的风险，为后面运维积累经验，或者如果没有风险，这个步骤也做到自愈流程中。


 NEONSAN-34 【recovery volume】正在recovery的卷无法停止：1.删除一个正在恢复的卷，提示正在恢复；2.增加停止recovery的CLI
 
 neonfio -name=case -ioengine=qbd -direct=1 -bs=1M -volume=vol/test  -config_file=/etc/neonsan/qbd.conf -rw=read -iodepth=10 -time_based -numjobs=1 -runtime=500 -use_tcp=1

 本周工作：
	1.完成NEONSAN-653[云平台易捷版]galera 集群在断电/恢复电力之后不能自动恢复：修改、验证：已经push，待review；
	2.完成NEONSAN-34 【v2.1.5】【recovery volume】正在recovery的卷无法停止：修改、验证：已经push，待review
	3.完成NEONSAN-665 neonsan stop_job优化，已经合入3.0；
	4.出ubuntu 18.04.1 v2.5.1 arm版本的包。
	
下周计划：
	1.熟悉了解《大规模集群方案》；
	
	
galera_cluster 数据库初始化未同步，一段时间内数据不一致的问题；
https://galeracluster.com/library/documentation/schema-upgrades.html

kill -9 mysql进程，查看集群状态

从10节点登录firstbox 172.31.45.2
172.31.45.10 express1ar01n00
172.31.45.11 express1ar01n01
172.31.45.12 express1ar01n02 

sync;date;echo 1 > /proc/sys/kernel/sysrq && echo b > /proc/sysrq-trigger



export BRANCH_NAME="qa"
export BUILD_PATH=/z0/yangxingxiang/qa
export BUILD_VERSION="debs-ubuntu_16.04.5"
export BUILD_ONLY="true"

当使用新版本qbd时： Store 与client 保持原来的zk 配置不变，升级结束后，老qbd通过qbd.conf 中的配置依然与当前rg 的local center 建立连接， 并且将zk ip list 和local leader center写入到一个本地文件中，下次open_volume时 直接从本地文件中连接zk 获取local leader center ; 若失败则连接上次保存的local leader center。
当使用的qbd版本不支持保存local leader center 和local zk 到本地文件时，老的qbd 每次都从qbd.conf 中获取qfcip 与local center进行连接。
当扩容了一个新rg 之后，老版本的qbd.conf 要访问新rg 上的卷时， 在通过qbd.conf 连接到local center 后， local center 检查卷不属于本资源组， 于是转发到global center , global再转发到对应的rg 进行处理。在这种情况下， 新版本的qbd将只在首次需要进行两次转发， 后续都可以从本地文件获取新的所属rg local center。 老版本的center 则每次都需要转发2次。

https://tenant.quanxiangyun.com/qingcloud/manage/tickets/1598
富春云prometheus的iops信息为0。检查集群状态正常，节点的node-exporter，获取curl 9100/metrics信息正常，prometheus都正常：
原因：统计iops的变量类型（sig_atomic_t）是32位的，运行时间长了可能会越界出现负数，monitor上报prometheus的时候出现这种异常时直接上报为0，并且monitor中直接把lastIoStat清理，没有把本次的结果赋值给lastIoStat，导致一直上报0.
修改代码：把本次的查询结果赋值给lastIoStat，出现负数的时候，属于增量计算的，也能正常吧结果上报到prometheus
临时的解决方法：依次重启rg下面节点的store进程（重启进程影响较大，谨慎操作）

https://boss.qingcloud.com/tickets/tk-mqtsiogds3l#
泉州移动neonsan混插环境，iops只有1.2k左右，在交付前请研发检查。tv1 403 307 099.   w8rn69
分析：之前看错了，iops 12k左右，没有预热的情况下是正常，按照最后一步测试，大致可以测出来30K左右的iops，符合验收标准，需要按照交付文档测试

https://tenant.quanxiangyun.com/qingcloud/manage/tickets/1627
青岛创恒    私有云  sanc 环境  neoncenter 剩一个节点 running，现在启动了三台节点的neoncenter，neonsan命令还是报错
FATA[0002] Failed to list ssd. reason:HTTP status:<nil>, reason:Get http://10.21.0.21:2600/qfa?op=list_ssd&refresh_capacity=false: dial tcp 10.21.0.21:2600: connect: connection refused 
虚拟机无法启动
客户着急，麻烦尽快上来看下

tv 1350 470 643 2cwg57
center的一个历史bug，在切换center时偶尔可能会阻塞，客户用的还是老版本
root@neonsan41:~# neonsan list_version
NeonSAN Cli Version: 2.2.0 build:82399ce-Jan 10 2020 01:21:36
+---------+-------------------------------------+---------------------+
| NEONSAN |            BUILD VERSION            |    UPDATED TIME     |
+---------+-------------------------------------+---------------------+
| center  | 2.2.0,build:359eb7e-20200110.012237 | 2020-01-16 22:25:24 |
| monitor | 2.2.0,build:67e0050-20200110.012237 | 2020-01-16 23:14:28 |
| store   | 2.2.0,build:82399ce-20200110.012237 | 2020-01-16 22:35:47 |
+---------+-------------------------------------+---------------------+

https://track.yunify.com/browse/NEONSAN-163
https://track.yunify.com/browse/NEONSAN-363

https://boss.qingcloud.com/tickets/tk-hukrs78s2qa#
用户虚机挂载的vos盘 目录有段时间无法写入，排查系统日志，有这个报错，这个报错的触发原因需要协助排查一下

问题原因：网络异常引起center切换，当前版本存在一个bug，job队列超了会引起center切换失败，建议升级到新版彻底解决。

https://boss.qingcloud.com/tickets/tk-hukrs78s2qa# 338 137 162  1qaz@WSX   teamviewer
用户虚机挂载的vos盘 目录有段时间无法写入，上周五也有同类问题，期间持续了将近一个小时

root@neonsan41:~# cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-08-25 16:23:42.222] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-08-25 17:08:42.099] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-08-28 17:39:02.951] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 09:55:45.765] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan41:~# ssh 10.21.0.11
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-39-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
New release '18.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Thu Sep 10 13:38:19 2020 from 10.21.0.2
root@neonsan11:~#  cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-04 23:57:00.439] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:44:39.874] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:47:18.261] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:04:57.815] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:18:34.303] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:19:45.681] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:21:46.671] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:26:01.882] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan11:~# ssh 10.21.0.31
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-39-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
New release '18.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Wed Sep  9 09:54:37 2020 from 10.21.0.31
root@neonsan31:~#  cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-05 00:23:34.631] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 09:49:35.607] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan31:~# exit
logout
Connection to 10.21.0.31 closed.
root@neonsan11:~# ll /var/log/* -h^C
root@neonsan11:~# cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-04 23:57:00.439] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:44:39.874] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:47:18.261] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:04:57.815] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:18:34.303] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:19:45.681] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:21:46.671] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:26:01.882] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275


https://boss.qingcloud.com/tickets/tk-fprwho2yprd#
泰康保险 私有云 neonsan服务器重启问题排查
检查过系统日志看不出由于软件导致的重启。需要驻场同事装一下impitool 收集硬件信息


https://boss.qingcloud.com/tickets/tk-3njr9cn4bc4# 上海科创测试neonsan环境，客户反馈在使用iscsi时，有时候会在3点和23点左右连接iscsi中断。检查kern日志io 有delay，继而检查center日志，发现当时在删除snapshot，但是都删除失败了，发现只有19和29这2台删除失败，到19和29检查store日志发现有持续的SLOW IO告警，iostat看磁盘没有到瓶颈，负载也不高，麻烦帮忙看下原因


https://boss.qingcloud.com/tickets/tk-dobjwzaim99
私有云  上海科创
21 22节点neonstore服务异常退出，麻烦看下
tv12        1 421 688 504           isy584

科创这个问题在2.5.2解决了，在store节点连接其他store的时候，用select的方式设置了连接的超时时间，当select监听的套接字fd大于1024时，就会造成函数栈帧内的内存溢出，所以退出函数的时候crash。已经fix掉


本周工作：
1.完成升级脚本jira单NEONSAN-689、NEONSAN-691 修改验证，代码已经merge，升级脚本v1.12已经发布归档；
2.完成NEONSAN-669、NEONSAN-670、NEONSAN671、NEONSAN-672修改，代码已经push，待review；
3.工单处理和跟踪，目前本周有5个工单，4个已经有结果，1个还在分析中；
4.发布v2.5.2的版本；
5.解决Jenkins打包第三方库没有更新到最新节点的问题；

下周计划：
1.大规模集群方案优化中lib common的适配修改；
2.修改jira单；

http://old-releases.ubuntu.com/releases/16.04.5/


cd /root/debs-ubuntu_16.04.5/ && dpkg -r pitrix-neoncenter-dbg pitrix-neonsan-monitor pitrix-neonsan-tool pitrix-neonstore-dbg && dpkg -i pitrix-neoncenter-dbg-3.0.0.0.0.1.deb pitrix-neonsan-monitor-3.0.0.0.0.1.deb  pitrix-neonsan-tool-3.0.0.0.0.1.deb pitrix-neonstore-dbg-3.0.0.0.0.1.deb && supervisorctl stop neoncenter neonmonitor neonstore && supervisorctl start neoncenter neonmonitor neonstore


本周工作：
1.分析青岛创恒 center切换为主之后卡住，引起center切换的原因是center连接不上zk，分析zk异常的原因，可能是存在的bug；https://issues.apache.org/jira/browse/ZOOKEEPER-1582
2.完成NEONSAN-715变量越界，iops显示为0的修改和验证，代码已经merge；
3.完成NEONSAN-701 API list_volume支持互斥组名过滤的修改验证，代码已经push，待review；
4.完成NEONSAN-113 API list_store 支持rg_id rg_name,rg_label过滤；
5.完成NEONSAN-264 list_replica_location 支持更多查询选项，代码已经push，待review；
6.完成lib common中open_volume_common流程中获取qfcip流程适配修改的测试，此修改方法可能对一个节点对接多套neonsan（即一个节点多个qbd.conf）的场景不支持；


下周计划：
1.修改jira单；

ubuntu iso下载地址：
http://old-releases.ubuntu.com/releases/16.04.5/


open_volume_comcom的查询qfcip流程适配修改：
原来流程：通过/etc/neonsan/qbd.conf获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询得到qfcip，
修改成如下：
1. /etc/neonsan/local_zk_qfc.conf存在，且local_zk ip local cluster_name 和qbd.conf相同：则用/etc/neonsan/local_zk_qfc.conf查询，否则用/etc/neonsan/qbd.conf查询
（1）.从 /etc/neonsan/local_zk_qfc.conf 获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询（流程和原先的不变）
（2）.如果（1）查询失败，从/etc/neonsan/qbd.conf查询（查询流程和1一样）
（3）.如果是通过/etc/neonsan/qbd.conf查询成功的，把zk ips和查询到的qfc IP保存到/etc/neonsan/local_zk_qfc.conf中


本周工作：
1.NEONSAN-684分析，3.0.0已经不会在zk 里面写uuid，问题不存在；2.5.x版本中一般remove ssd应该也不会出现zk存在、数据库中不存在的情况，除非从zk中删除的时候异常，没有return，然后又把数据库中的删除，因此解决此问题需要从源头解决，即，从zk删除异常时返回失败，不继续从删数据库删除。
2.open_volume_comcom的查询qfcip流程适配修改和验证，可能和最初的需求理解有偏差，还需进一步分析设计；
3.完成ubun18.04版本包的自动化；
4.neonsan容器化部署研究，初步想了一些镜像打包的思路和方法，下一步还需要参考业界分布式存储容器化的方案、容器team团队给的赋能和经验分享；
5.v3.0.1、v2.5.5的打包工作，并且出手动出v3.0.1使用openssl库的打包；

下周工作：
1.继续neonsan容器化部署研究；
2.open_volume_comcom适配的分析、设计、修改；


本周工作：
1.完成NEONSAN-544修改验证，代码已经push，待review；
2.完成NEONSAN-764修改验证，代码已经push，待review：
3.完成NEONSAN-593修改验证，代码merge
4.完成NEONSAN-801修改验证，代码merge；
5.完成NEONSAN-766修改验证，代码已经push，待review：
6.完成v2.5.5版本发布相关材料准备和归档，v3.0.1打包相关工作。

下周工作：
1.继续neonsan容器化部署研究；
2.open_volume_comcom适配的需求澄清，修改、验证；


open_volume_comcom的查询qfcip流程适配：
背景：后续的大规模部署新架构将会是一个global center 管理多个local center , 每个local center 集群负责它所管理的rg的卷的错误处理和open volume等操作，这次需求的适配为了兼容后续超算项目从3.x 版本升级到新架构的场景。期望当使用新版本qbd时，Store 与client 保持原来的zk 配置不变
原来流程：通过/etc/neonsan/qbd.conf获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询得到qfcip，
修改成如下：
1. /etc/neonsan/local_zk_qfc.conf存在，且local_zk ip local cluster_name 和qbd.conf相同：则用/etc/neonsan/local_zk_qfc.conf查询，否则用/etc/neonsan/qbd.conf查询
（1）.从 /etc/neonsan/local_zk_qfc.conf 获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询（流程和原先的不变）
（2）.如果（1）查询失败，从/etc/neonsan/qbd.conf查询（查询流程和1一样）
（3）.如果是通过/etc/neonsan/qbd.conf查询成功的，把zk ips和查询到的qfc IP保存到/etc/neonsan/local_zk_qfc.conf中




#define MAX_BUFFER_SIZE_128  128
#define MAX_BUFFER_SIZE_1024  1024


struct qfa_options {
    char userkey_name[MAX_BUFFER_SIZE_128];
    char userkey_passwd[MAX_BUFFER_SIZE_128];
    char zk_ips[MAX_BUFFER_SIZE_1024]
    char center_ips[MAX_BUFFER_SIZE_1024];
}

int open_volume_common(struct qfa_client_volume *vol, int lib_ver, struct qfa_options *options);


lib common适配修改：
1.options->zk_ips, options->center_ips没有内容时，open_volume_common按照原流程查询qfcip；
2.options->zk_ips, options->center_ips有内容时，open_volume_common使用options->zk_ips, options->center_ips查询qfcip；
3.查询到qfcip，向leader center发送open_volume（此步骤不涉及改动）；
4.解析open_volume返回的卷的属性，把所属的zk_ips,center_ip保存到options->zk_ips, options->center_ips中；

center修改：
1.OpenVolume API输出中增加参数zk_ips，center_ips；
2.两参数直接从该节点的/etc/neonsan/center.conf中读取zookeeper.ip,center.mngt_ip；

qbd修改：从缓存读取到内容（读取不到赋值空）后调用open_volume_common，调用open_volume_common完后从options->zk_ips, options->center_ips读取内容进行缓存；



2020年10月23日讨论纪要问题：
1.qemu qfa_open_volume的缓存问题；
2.qbd client老版本不支持时，open_volumem每次都需要global转发，即：qbd-> global -> local， local-> global->qbd；
3.center_ips缓存多个，leader center ip放在第一个；

引起同步时间太长导致会话超时的因素，1、网络，2、磁盘io，3、ticktime
网络都是不可靠的，不能通过方面去解决，只能通过磁盘，ticktime可以考虑调整，从切换center时间中去取舍
另外我现在想在本地中看能不能复现出来



本周工作：
1.完成open_volume_comcom适配的需求分析，澄清，代码开发完成，待测试，总体完成60%；
2.分析浦发硅谷银行zk不稳定导致临时节点消失问题：zookeerper 服务在同步日志过程中耗时太长，花了9465ms，网上查找相关案例，同步日志ZK无法响应外部请求，即无法检测到心跳，进而引发session过期，session过期会引发zk临时节点的消失，根据日志查看zk源码，代码中只是简单的把事务日志刷入磁盘，不涉及到节点之间的的网络交互，却耗时9000多ms，因此在本地环境复现，在zk的节点，给系统盘增加io压力，但是没有出现fsync-ing同步超时的告警，于是通过修改zk代码复现，leader 节点出现fsync这个会出现session超期，follower节点出现fsync概率性出现follow er与leader连接断开。

下周工作：
1.继续neonsan容器化部署研究；
2.继续open_volume_comcom适配的需求、开发验证；
3.3.0.1版本打包、准备发布材料等相关工作；






本周工作：
1.完成open_volume_comcom适配的自验证，qbd进行适配后在整体测试下，总体完成90%；
2.完成NEONSAN-813开发验证，代码已经merge；
3.完成NEONSAN-836 NeonSAN自动化打包时生成每个包的md5值开发，代码已经merge；
4.主要考虑后续neonsan容器化部署时zk、mysql数据需要持久化，学习k8s容器持久化存储中PV PVC StorageClass相关技术特效；
5.完成3.0.1版本发布相关材料准备归档，3.0.2打包，v2.5.5,V3.0.1手动生成每个包的md5值，已经归档到anybox；

下周工作：
1.bug 840的分析和修改;
2.继续neonsan容器化部署研究；

本周工作：
1.分析浦发硅谷银行导致磁盘IO压力的原因，补充了一些可能引起磁盘io压力过大的原因和完善故障报告；
2.完成NEONSAN-840开发验证，代码已经push，review有一些检视意见，待修改；

下周工作：
1.qbd open_volume_comcom的验证;
2.neonsan容器化部署研究；


SELECT s.uuid, s.free, t.threshold FROM ssd s, (select CEILING(AVG(free)*0.9) AS threshold FROM ssd, store WHERE store.status='OK' AND ssd.status='OK' AND ssd.store_id=store.id AND store.rg_id IN (SELECT store.rg_id FROM store, ssd WHERE ssd.store_id=store.id AND ssd.uuid='09f55ec5-a359-4390-af1b-f21c549fe533')) t  WHERE s.uuid='09f55ec5-a359-4390-af1b-f21c549fe533';


1)肯定的是升级过程不会冲掉个性化配置。zookeeper 的问题在升级之前即存在，只是没有引发NeonSAN故障。
2)我们在排查时结合zookeeper的源代码和日志，发现故障的直接原因是写IO超时，这个环境是融合部署环境，物理节点上既有zookeeper，NeonSAN存储服务，NeonSAN监控，又有虚拟机在运行，其中zookeeper,NeonSAN 日志，以及监控都会在线往系统盘写日志，因此会存在相互影响的情况。通过对比EQ的环境，EQ的环境负载CPU、内存资源使用率比较小，日志没有出现过fsync-ing同步时间超长的告警，而GD环境负载一般都是70%以上，日志中经常出现fsync-ing同步时间超长的告警。
3)zookeeper的最佳实践也建议将zookeeper作为独立集群进行单独部署，当前的融合部署模式不符合zookeeper的最佳实践要求。官网中关于磁盘性能对zookeeper写事务的影响和dataLogDir的配置建议说明：http://svn.apache.org/repos/asf/zookeeper/trunk/docs/zookeeperAdmin.html：

这个我想这样改：
rebalanceSsd函数中循环调用policy.ChooseAltReplicaPlace查询，首次调用的时候第三个参数restrict.ExcludeStoreList空，查询的replicaPlace经过后面检查如果不满足，把replicaPlace对应的storeId加到restrict.ExcludeStoreList再次查询，直到查询满足条件的或者查询失败的结束，查到满足条件的可以迁移，失败则不能迁移



neonsan create_volume --volume poo/vol0  --size 2T  --repcount 2 --thick_prov 

1.构造集群中如下场景：
domain1 store 31 43 5个ssd 剩余大概800G,  800G平摊到5块ssd中
domain2 store 41    2个ssd 剩余大概700G， 700G平摊到2块ssd中,
domain3 store 42    2个ssd 剩余大概100G， 100G平摊到2块ssd中,

2.选择domain3 store 42一块使用率较高的ssd，rebalance_ssd，此时 domain store ssd的排序是：
domain1 43 ssd1
domain1 31 ssd2

domain2 41 ssd1


现在会先尝试迁移到domain1 43 ssd1，domain1 31 ssd1,不满足，最后迁移到domain2 41 ssd1


neonsan remove_ssd --ssd_uuid 6f99432e-42d8-4fd0-ba1e-003d38f4e2bd  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid e9c84436-c130-47b0-bb3a-48fe458fbed6  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 2dfbb532-b24c-4fbb-8262-42975485d6ab  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 9ed4c7d5-7d01-4f8d-9fec-6d1d9bf607c9  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 10ada5e3-bfa3-44d1-9556-cd9f039d2abc  --evacuate_data no
sleep 10

neonsan remove_ssd --ssd_uuid c9701ff0-36c0-422c-b7af-838bcb3522f5  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 45e0d44e-287f-49cc-a06f-1438fcad06e9  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 3fe15523-c355-4426-ab2d-f2f5136d0a53  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid f4e41ae4-8f7d-473c-82d1-a2701d4c7d52  --evacuate_data no


neonsan rebalance_ssd --ssd_uuid f29fa52d-6d26-4b9f-8d89-72fff948fa48


本周工作：
1.qbd适配完成后，大规模部署open_volume_common的整体测试，没有发现什么问题，代码已经push，待review：
2.完成NEONSAN-840场景未考虑全的问题；
3.Neonsan需要支持centos8.2，centos8.2版本的打包，总体已经完成%60；


下周工作：
1.centos8.2版本的打包、相关驱动下载和验证；

git clone https://github.com/Open-CAS/open-cas-linux.git
cd open-cas-linux
git checkout origin/v20.3.3
git submodule update --init
./configure
make
make install

本周工作：2020-11-23 -2020-11-29
1.工单处理：
    1.私有云华创：一块已经删除的硬盘为降级状态告警  --告警残留，按照 https://cwiki.yunify.com/pages/viewpage.action?pageId=23670454处理，第二天向付裕确认，问题解决；
    2.私有云富春云：客户自己搭建的一台redis主机，在今天上午10:42:39发生了主从切换 --查看Neonsan相关日志，无相关异常日志，应该不是neonsan存储问题引起；
    3.私有云滕州市政府：早上有部分卷降级触发了qbd error告警--当时磁盘io压力大，aio_post和aio_comp的个数不一样，可能就是盘没处理完，会引起卷降级；--看下日志是否有COW
    4.川庆砖探：这个环境又断电后 卷降级了  recovry失败--rose帮看，recovery失败时因为磁盘空间不足引起，解决措施：gc了一下，现在打开auto_balance，先均衡，然后recovery成功；
    客户通过iscsi连不上neonsan环境--改了SCST的配置，，客户连上了卷
    5.泉峰集团一个环境有slow io 大量的告警 -- 按照https://cwiki.yunify.com/display/NeonSAN/05-SLOW+IO发现是save_metadata引起，何智已经修改配置参数优化，暂时没有出现告警；-补充作何改动，现在环境是否保留改动？
    6.英大人寿，有2台虚拟机开机失败，iaas这边报 qbd 挂载超时 30s
    1.看syslog日志open_volume_common[vol/vosi-d7u0x9we.img:/etc/neonsan/qbd.conf]:28 错误码2；
    2.查看当时center日志，发现center日志也收到open volume的消息，但是后面没有响应的日志了；
    3.查看center日志数据库盘满了导致的，导致请求都在执行InsertAuditRecord的位置卡住了，所有的请求都没有得到处理。

    改进措施：
    1.清理的大的日志文件alertmanager.log；
    2.在logrorate中配置alertmanager.log日志切割转存；防止再出现空间爆满--已经实施
    7.英大人寿问题2：用户给数据盘做个备份，center日志中报空间不足--排查发现是alertmanager.log日志过大空间占满，解决错误清理日志，配置上logrotate日志切割转存；
    8.泰康私有云:老版本的neonsan （1.0.9） 一个 dw 管理节点（只有zookeeper和mysql） 根文件系统只读了，需要重启，能帮忙检查一下，是否可以直接重启?-- 检查两个节点的zk和myql的是正常的，这个节点可以直接重启，硬盘和raid 卡都检查过，没有发现问题
    9.sh1a公有云：vos-95w11bvd没有创建成功--NeonSAN收到的，和反馈失败的卷名字不一样，通过NeonSAN测试了一下创建卷，正常的；
     是哪个用户创建的？
     usr-gFS5392T，通过备份创建mysql，然后卡在创建硬盘了
     可能是还在导数据？
     感觉是盘太大，job超时了，可以加大 创建的 超时时间
     snapshot恢复成vos花了 15 小时；
     从snapshot节点 获取数据解压 并导入neonsan 。 一个管道进行。 没有限速。如果要优化的话，我觉得需要从neonsan入手。 分析neonsan import_diff 是否可以快
     do_create_volume_from_snapshot 这个函数import_diff的输入时export_diff出来的么？
     我们研究一下，看能否加速，在混插环境的 import_diff？
     qbd从2.1.0版本支持挂载volume snapshot，如果比较好的多线程读写工具，也可以换种思路导出。import_diff 如果能实现肯定是最快的
     应该是可以实现的
     备份如果只用快照实现，有什么问题吗？备份链的话，性能影响大不大？友商基本都是快照，不想我们要导到外部；
     现在还不支持，需要后续开发
     备份基于快照实现，其实备份的数据还是在当前的存储设备上；
     怎么能登录执行 import_diff的节点呢？先分析一下性能慢的原因；
     硬盘创建好了，主机状态有问题，麻烦看下
     sh1a 巡检发现交控有个plus集群异常了 neonsan有做调整吗？报的是文件系统的错误
     这是neonsan volume 吗
     是的
     没有做过
     mysql 无论如何不可能造成文件系统故障的，iaas 也不会
     通过qemu后端挂载的neonsan volume， qemu日志里没有报错
     这个待会得找一台，先做snapshot，然后用xfs_repair修复，但三台都坏，非常蹊跷
     vm内部也没有io error，第一次报错时间基本一致，
        Nov 27 19:07:46 i-m7voa7sn
        Nov 27 19:07:45 i-n5dh1g5b
        Nov 27 19:07:45 i-0wxq90b5
    我刚刚联系上客户了  这个是基于他们的集群备份创建的新集群
    那意思就是通过备份创建的盘是坏的
    是的
    客户允许我们重启 我先重启一下
    那跟下午的问题是一回事吧
    是的
    重启了也一直报那个错；
    得repair，xfs error不会自己就好了；
    这个有备份 可以直接修复；
    先打个快照
    客户是基于快照来创建的集群 已经有备份了
    我这边操作不了 从vm里 没办法umount
    得把mysql停掉，lsof /data没有占用了再试试
    mysql都没有启动，之前看mysql的日志是这样子的
    lsof /data可以看到，我在操作i-0wxq90b5，得从db应用层面停，杀掉之后还会起来
    kill掉几个进程以后 可以umount，我在修复i-m7voa7sn
    i-0wxq90b5:修完挂载不报了
    我操作下
    可以了，你重启下集群再看下还报错么
    好的，有问题，mysql起不来
    说明备份恢复过程里面有数据异常
    @Ada 如果客户基于备份重新创建一个集群 能解决吗？db那边不好修复 而且数据没了
    原集群还在吗
    在的
    原集群的卷是哪个？
    我的意思是这个备份很可能就是坏的。
    cl-vo5fqdzh，源集群是这个；
    那这个集群还是好的吗？
    是的，我明天联系客户 让客户重新创建一个备份，然后试下
    为什么非要通过备份创建，不是有克隆吗？
    mysql plus没有克隆，只能基于备份创建新的
    导入的过程中打了很多snapshot，不过应该是每次导入一个备份之后就删掉了，后续可以把这些snapshot留着，如果最后一个启动不了，可以rollback试一下
    明天做备份得做全量备份，再创建？
    vol/vos-hrdg2z6s_ss-gkvu9ko8#vos-95w11bvd_.img 这个卷名的意思是 vos-95w11bvd 是卷vos-hrdg2z6s的备份卷？
    不是，z6s是原盘，ko8是快照ID，在用这个快照生成最终卷 bvd；
    嗯，明白了，看到备份过来的卷数据跟原卷大小有些差别，那可能是这个snapshot之后原卷又有写入；
    肯定有写入，明天看全量备份有没有异常吧
    好的
    交控根据备份创建新集群失败了，今天根据备份: ss-opoydyll重新创建了一次集群，还是失败，麻烦看下
    还是失败指的是什么失败呢，文件系统不可用吗？
    创建集群job失败了，所以运维账号没创建
    那你得找到集群job失败的原因；
    到原因了，是因为这个节点重放redo比较慢，用时6分，其他节点用时2分钟，由于该节点没有重放redo、迟迟没有加入集群，因此这个节点restore时没走完全部流程，其实带来的影响主要是运维账号没初始化，我们较新版本是可以自动订正的，即使restore指令失败了，集群状态也会自动订正。db同事已经帮客户把这个集群修复好了
    
    10.泉州移动私有云# 迁移数据时，一个volume 降级：原因跟480应该是一样的，本质上都是之前的open volume hung住了，在volume set新meta version之后这个open volume请求才到达store，store上面新的meta version比这个openvolume里的新，所以报这个错，480在2.5版本里面已经fix了
    11.客户前端业务使用sanc，后端oracle rac  使用独立neonsan全闪存储， 客户反馈这周访问oracle比较慢。能否麻烦帮忙一起检查下 咱们2个存储池的性能，确定没问题，让客户找应用方再排查下业务 ：https://tenant.quanxiangyun.com/qingcloud/manage/tickets/2430
    检查网络没有延迟，日志没有slow io，何智帮忙检查：确认了是 SQL 的问题，跟Neonsan没关系。
    
    
    
    
     

2.Neosan支持centos8.2：
    1.编译脚本，rpm包的编译整合成一套，代码已经push;
    2.SCST驱动编译打包，已经编译成功；
    

下周任务：
    1.Bug修改和验证；
    2.neonsan容器化部署研究；
    



周四上午10：00准备review一下bitmap的最新设计文档以及代码，大家时间有没有冲突的？


设计文档：https://yunify.anybox.qingcloud.com/f/3682955
https://git.internal.yunify.com/SAN2.0/qfcenter/commit/dde9c4dcac08d481bfd40e4ecc6d3bba9f12f556
https://git.internal.yunify.com/SAN2.0/qfa/commit/c15cd87d111d47570cd0a5a118d0fd7e89331840

09-31204-750025-997324


systemctl stop firewalld
systemctl disable firewalld
/usr/sbin/sestatus -v
getenforce
sed -i 's/enforcing/disabled/' /etc/selinux/config
setenforce 0
/usr/sbin/sestatus -v
swapoff -a 
sed -ri 's/.*swap.*/#&/' /etc/fstab

cat >> /etc/hosts << EOF
192.168.101.2 k8s-master
192.168.101.3 k8s-node1
192.168.101.4 k8s-node2
EOF

yum install ntpdate -y

cd /root/debs-ubuntu_16.04.5/ && dpkg -r pitrix-neoncenter-dbg pitrix-neonsan-monitor pitrix-neonsan-tool pitrix-neonstore-dbg && dpkg -i pitrix-neoncenter-dbg-3.0.2.0.0.1.deb pitrix-neonsan-monitor-3.0.2.0.0.1.deb  pitrix-neonsan-tool-3.0.2.0.0.1.deb pitrix-neonstore-dbg-3.0.2.0.0.1.deb && supervisorctl stop neoncenter neonmonitor neonstore && supervisorctl start neoncenter neonmonitor neonstore


本周任务：
1.v3.0.2版本打包、归档、相关材料的准备；
2.neonsan支持centos8.2代码入库后，在jenkins自动化出包的测试，发现一个脚本无执行权限的问题已经解决；
3.补充open_volume_common适配的测试场景；
4.熟悉kubsphere容器平台，熟悉在其平台上部署mysql、创建任务等操作。

    

下周任务：
    1.Bug修改和验证；
    2.继续熟悉使用kubsphere容器平台；
    
    
最近涉及到KA客户以及公有云的环境改造和优化项目大约有下面几项，需要大家跟一下，把每个项目分配到个人了，后面有其他的支持再轮流安排。 注意，这些具体实施由服务团队同事负责，我们要帮助他们确认好方案，以及解决实施过程中遇到的技术问题。
富春云混插环境的优化调试（方案已经提供，需要跟客户讨论）素香
浦发硅谷环境的整体改造（已经草拟方案，需要研发参加评估）兴祥（远志支持）
泰康老版本存储的升级（离线还是在线方案还在等待客户确认）国武，佳宁
泰康需要2位研发保持工作日和非工作时间手机畅通 国武，佳宁
公有云RDMA适配 马强

admin/Zhu88jie

本周工作：
1.大规模部署open_volume_common适配检视意见的修改；
2.熟悉kubsphere容器平台：
（1）在其平台上创建企业空间、项目、帐户和角色；
（2）在其平台上创建存储卷；
（3）在其平台上部署自制的应用，应用下面添加服务组件，服务组件分成有状态的服务和无状态的服务；Neonsan的容器化部署，我的理解实际上就是部署一个Neonsan的应用，neonsan组件包括：zk、mysql、center、store、monitore，其中zk、mysql是用状态的服务，即需要持久化存储一些数据。
3.硅谷环境的整体改造方案分析和zk在线扩容和减容的验证；



下周工作：
    1.Bug修改和验证；
    2.继续熟悉使用kubsphere容器平台；
    
    
本周工作：
1.Neonsan容器化部署：
（1）制作Ubuntu1604基础镜像；
（2）基于Ubuntu1604基础镜像制作zookeeper的镜像；
（3）基于Ubuntu1604基础镜像制作galera mysql的镜像；
（4）基于Ubuntu1604基础镜像制作center的镜像；
（5）基于Ubuntu1604基础镜像制作store的镜像；
（6）基于Ubuntu1604基础镜像制作monitor的镜像；       (7)验证制作出来的镜像容器化部署：目前容器化的网络网络模式host的，网络与物理机使用相同的网络空间，网络不隔离。store的容器需要把磁盘直通到容器里面，目前除了monitor不能单独容器部署（原因是monitore依赖store的配置。并且monitor需要通过ssh远程查询zk和mysql的状态），其他功能正常。

2.V3.0.2版本重新打包归档；



下周工作：
    1.验证浦发硅谷应该数据库导出后导入到新库后数据的完整性；
    2.继续容器化部署研究；
    

本周工作：
1.Neonsan容器化部署与容器团队的石永红交流讨论，其解答了一些疑问和概念上的问题，并给了一些接接下来容器化部署的步骤：
  （1）把容器的配置写成一些yaml文件，用kubectl apply -f yaml的方式把neonsan部署和启动；
  （2）把这些yaml打包成helm chart，把neonsan做出可以放到应用商店的应用；
2.完成ubuntu20.04 neonsan安装包的自动化；
3.完成v3.0.3的bug NEONSAN-917修改和验证；
4.浦发硅谷数据库导出数据的完整行检查，首次导出的数据没有包含neonsan相关的函数和过程，加-R参数再次导出已经包含
  


下周工作：
    1.继续容器化部署研究；



试用期总结1.工作内容及工作成果 ：
1.参与neonsan开发和维护：（1）开发支持stop evict_store、rebalance_ssd、recovery_volume的功能特性，完成30+的bug的修复；（2）完成大规模部署open_volume_common的适配开发；
2.负责neonsan的编译打包和支持新的OS版本、以及neonsan版本的发布。（1）新增支持ubuntu18.04.5、centos8.2、ubuntu20.04 OS的neonsan版本，并且已经实现自动化；（2）优化编译脚本，编译脚本统一，使开发人员都能使用本地脚本编译打包版本，极大提高项目团队的工作效率；（3）负责V2.5.0-V2.5.4、V3.0.0-V3.0.2 9个版本相关材料的准备和发布；
3.负责galera数据库的开发和维护：（1）解决了galera_cluster集群全部宕机，出现缺少/data/galera/mysql/mysql无法自愈、galera_cluster集群宕机两个节点，节点恢复后集群无法自愈等4个BUG；
4.负责zookeeper相关问题定位和分析：（1）分析定位浦发硅谷银行临时节点消失引起故障的原因，并提出相应的解决优化措施；（2）青岛创恒 center切换为主之后卡住，引起center切换的原因是center连接不上zk，分析zk异常的原因；（3）整理和验证zookeeper扩容和缩容的方案；（4）整理和验证Xenondb数据库迁移到galera数据库的方案；
5.工单处理：处理工单15+个；
6.负责neonsan容器化部署：目前已经完成各个组件的镜像制作、镜像已经在docker中部署验证通过；

2.下一步的工作计划：
 1.把容器化部署的配置写成一些yaml文件，用kubectl apply -f yaml的方式把neonsan部署和启动；
 2.把这些yaml打包成helm chart，把neonsan做出可以放到应用商店的应用；
 3.参与neonsan开发和维护，更加熟悉neonsan功能特性和优势，比如极短IO路径，为neonsan设计和开发更有竞争力的功能特性，为neonsan商业化更加成功贡献自己的一份力量
试用期总结填写提示试用期总结内容必须包含以下两项：1.工作内容及工作成果 2.下一步的工作计划
对公司的建议1.公司代码规范化，制定C、 C++、go等语言的编程规范，所有相关的研发部门都遵守同一种规范，利于代码的开发和维护；
2.加强CICD持续集成、持续交付流水的建设，比如所有研发部门使用同一套流水线平台，各个部门的CICD经验可以得到分享，从而提高开发效率；
3.部门内加强各种技术经验的分享和交流活动，比如每两周技术经验分享或者编程技巧、算法的分享，加强工程师文件建设。


现在升级脚本问题：如果节点较多，没升一个节点需要输入节点IP，容易输错或者忘记升级到哪个节点，有可能会重复升级或者漏升级节点

修改方案1：能够一键式完成升级，正常的话不需要多次输入节点IP
1.升级center时，获取到center的IP_LIST,然后遍历IP LIST按照现在流程升级，如果是leader center，跳过，待其他follower节点升级完最后再升级center leader
2.每升级成功一个节点，记录下升级成功的节点，如果过程中升级异常，退出，排查解决后，继续按照步骤1升级，然后遍历IP LIST判断是否已经升级，已经升级的直接跳过；
3.store的也是同样的方式


保守一点的修改方案2：
升级流程和1一样，只是升级加入一个交互式的人为确认（y/n）是否继续升级下一个节点，可以人为检查升级过程是否有异常打印，也不需要每次输入节点IP。

NEONSAN-939问题引入回溯：
问题原因：
1.最初count返回的满足过滤条件的总数，701的需求加上支持group_name过滤后，因为对pn count的理解不完全，返回把pn count也当做一种过滤后卷的数量；
2.修改完成后和portal联调不充分；


修改：count返回除了pn、count，满足所有过滤条件的卷的数量；


本周工作：
1.Neonsan容器化部署与容器团队的石永红交流讨论，其解答了一些疑问和概念上的问题，并给了一些接接下来容器化部署的步骤：
  （1）把容器的配置写成一些yaml文件，用kubectl apply -f yaml的方式把neonsan部署和启动；
  （2）把这些yaml打包成helm chart，把neonsan做出可以放到应用商店的应用；
2.完成ubuntu20.04 neonsan安装包的自动化；
3.完成v3.0.3的bug NEONSAN-917修改和验证；
4.浦发硅谷数据库导出数据的完整行检查，首次导出的数据没有包含neonsan相关的函数和过程，加-R参数再次导出已经包含
  


下周工作：
    1.继续容器化部署研究；
    
    

本周工作：
1.Neonsan容器化部署与容器：
（1）熟悉了解rook的架构：rook包含自身的两个pod：rook-discover,rook-ceph-agent，其他存储自身的pod；
（2）了解kubernets yaml的格式，尝试把zk的部署写成yaml配置，用kubectl apply -f yaml部署，遇到不使用本地镜像制作的镜像而是pull远端镜像的问题，在石永红帮助下已经解决；
  
2.浦发硅谷迁移改造：已经梳理出zk迁移、数据迁移的详细步骤，并且与黄蔚然沟通细节，整个项目实际的操作细节黄蔚然在元旦节整理出来，然后组织review；
3.完成bug NEONSAN-939修改和验证，安装包已经更新到portal 周杨环境，待其确认；
4.完成升级脚本每升级一个store需要输入节点IP问题的优化；代码已经push，待review；
  

下周工作：
    1.继续容器化部署研究；
    2.继续跟进浦发硅谷迁移和组织文档review；
    
    

本周工作：
1.浦发硅谷迁移改造方案文档编写、测试验证；
2.解决centos8.2 neonsan安装包无法解决的问题；

下周计划：
1.继续容器化部署研究；
2.浦发硅谷支撑；

export PROXY="http://192.168.101.231:8990"
export PROXYS="https://192.168.101.231:8990"
export http_proxy=$PROXY
export https_proxy=$PROXYS
export HTTP_PROXY=$PROXY
export HTTPS_PROXY=$PROXYS

mount -t nfs 192.168.101.10:/z0 /z0



本周工作：
1；NEONSAN-943容器化部署NeonSAN:完成zookeeper的部署，正在进行galera数据库的部署，总体进度完成50%；

下周计划：
1.继续容器化部署；
2.浦发硅谷迁移支撑；

2020年年度工作总结：
1.专业技能、岗位核心能力

https://git.internal.yunify.com/SAN2.0/qfa/merge_requests/1398/，素香，NEONSAN-943 容器化部署NeonSAN 全部搞完了，包括portal 监控等相关的，相关的配置和脚本可以直接合入qa不？还是你看下？ 目前neonio的镜像我是直接用v3.0.4的安装包来制作的，改动了几处的代码，但是在本次MR没有合入，这部分我打算

 # This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

auto eth6
iface eth6 inet manual
bond-master bond0

auto eth7
iface eth7 inet manual
bond-master bond0

auto bond0
iface bond0 inet static
        address 192.168.101.175
        netmask 255.255.255.0
        broadcast 192.168.101.255
        gateway 192.168.101.1
        bond-mode 4
        bond-miimon 100
        bond-slaves eth6 eth7

auto eth4
iface eth4 inet static
        address 10.10.88.175
        netmask 255.255.255.0
        broadcast 10.10.88.255

auto eth5
iface eth5 inet static
        address 10.10.99.175
        netmask 255.255.255.0
        broadcast 10.10.99.255
        
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

auto eth4
iface eth4 inet manual
bond-master bond0

auto eth5
iface eth5 inet manual
bond-master bond0

auto bond0
iface bond0 inet static
	address 192.168.101.174
	netmask 255.255.255.0
	broadcast 192.168.101.255
        gateway 192.168.101.1
	bond-mode 1
	bond-miimon 100
	bond-slaves eth4 eth5

auto eth6
iface eth6 inet static
	address 10.10.88.174
	netmask 255.255.255.0
	broadcast 10.10.88.255

auto eth7
iface eth7 inet static
	address 10.10.99.174
	netmask 255.255.255.0
	broadcast 10.10.99.255    

# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

auto eth6
iface eth6 inet manual
bond-master bond0

auto eth7
iface eth7 inet manual
bond-master bond0

auto bond0
iface bond0 inet static
	address 192.168.101.176
	netmask 255.255.255.0
	broadcast 192.168.101.255
        gateway 192.168.101.1
	bond-mode 4
	bond-miimon 100
	bond-slaves eth6 eth7

auto eth4
iface eth4 inet static
	address 10.10.88.176
	netmask 255.255.255.0
	broadcast 10.10.88.255

auto eth5
iface eth5 inet static
	address 10.10.99.176
	netmask 255.255.255.0
	broadcast 10.10.99.255   



本周工作：
1.配合neonio测试：1.跟杨泽凡/童进讲解neonio安装部署；2.给伍永红恢复174的容器测试环境；
2.适配海光CPU的安装包，后来发现海光CPU也是X86的，使用的是标准的centos7.5的标准OS，目前编译也是使用标准的镜像来制作docker编译环境，所以应该用重新编译打包，可以直接拿以前出的过包来安装测试；
3. 3.2 bug NEONSAN-684不用修改，分析NEONSAN-234的优化；

下周计划：
1.继续云原生容器化；
2.3.2 bug    

recovery.fast_recovery：
2.5.5版本应该就没有什么问题了，但是我们大的版本有的会滞后2.5.5版本，也就是3.0.0版本可能不包含2.5.5版本的fix，3.0.4应该是没问题的



本周工作：
1.编译打包ubuntu18.04 arm环境的安装包；
2.编译打包中嘉和信POC centos7.9环境的安装包；
3.工单处理，目前4个，两个是已知bug，一个文档咨询类，还有一个迁移任务进度显示问题，代码分析不出原因，复现不出来；
4.neonio需求文档和测试case的准备；

下周计划：
1.云原生容器化文档准备；
2.3.2 bug   


https://blog.csdn.net/u011708337/article/details/106051943/

本周工作：
1.NEONSAN-290：基于卷的性能监控；两种方案：（1）在store统计卷在该节点的io，center向store查询卷所有副本的io，monitor像center查询；（2）在client端统计卷的io；
研究华为Fusionstorage的实现，它有vbs模块，负责卷管理和io的接入和处理，他的实现肯定是属于在client端做的；
neonsan如果在client端做：(1).client是qbd，直接在client端部署monitor，monitor解析iostat、qbd -ld结果得到卷的iops和带宽，向promethus上报；
                        （2）client是qemu，在neonsan lib中统计io，定期打印到日志，同样需要部署monitor，monitor解析日志得到iops、带宽向promethus上报；
                        
2.川航alive消失的问题，store卡死10个小时没日志的复现，目前复现不了；


下周计划：
3.2 bug    

IaaS 给我们在公有云开放了一个后付费的公共子账号，可以用来搭建测试环境
ks@yunify.com#neonsan
apscohdnpnb5n1vcx8zbx12#4451nv8 

dd if=/dev/zero of=/dev/nvme0n1  bs=1M count=20480 oflag=direct
dd if=/dev/zero of=/dev/nvme1n1  bs=1M count=20480 oflag=direct 
dd if=/dev/zero of=/dev/nvme2n1  bs=1M count=20480 oflag=direct     
dd if=/dev/zero of=/dev/nvme3n1  bs=1M count=20480 oflag=direct




docker tag registry.aliyuncs.com/google_containers/csi-provisioner:v2.2.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.2
docker tag registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0
docker tag registry.aliyuncs.com/google_containers/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1
docker tag registry.aliyuncs.com/google_containers/csi-snapshotter:v4.1.1 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.1
docker tag registry.aliyuncs.com/google_containers/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0


docker pull registry.aliyuncs.com/google_containers/csi-provisioner:v2.2.2 
docker pull registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.2.0
docker pull registry.aliyuncs.com/google_containers/csi-attacher:v3.2.1
docker pull registry.aliyuncs.com/google_containers/csi-snapshotter:v4.1.1
docker pull registry.aliyuncs.com/google_containers/csi-resizer:v1.2.0


kubectl api-resources --verbs=list --namespaced -o name  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph

#!/usr/bin/env bash
DISK="/dev/sdb"

# Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)

# You will have to run this step for all disks.
sgdisk --zap-all $DISK

# Clean hdds with dd
dd if=/dev/zero of="$DISK" bs=1M count=100 oflag=direct,dsync

# Clean disks such as ssd with blkdiscard instead of dd
blkdiscard $DISK

# These steps only have to be run once on each node
# If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks.
ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %

# ceph-volume setup can leave ceph-<UUID> directories in /dev and /dev/mapper (unnecessary clutter)
rm -rf /dev/ceph-*
rm -rf /dev/mapper/ceph--*

# Inform the OS of partition table changes
partprobe $DISK  


本周工作：
1.NEONSAN-290：基于卷的性能监控方案，在client统计性能：
(1).client是qbd，直接在client端部署monitor，monitor解析iostat、qbd -ld结果得到卷的iops和带宽，向promethus上报；
(2).client是qemu，部署monitor通过qemu命令查询到卷的io，向promethus上报；
                        
2.neonio的性能测试分析：（1）.物理部署时数据和管理的网络是分开，现在neonio默认是共用的，这个肯定影响, （2）是感觉增加pod这一层后，网络性能可能编差；(3).节点重启后zk概率出现选举失败，无法正常工作，通过加大tickTime的时间也还会概率出现，怀疑有两点：（1）是容器化部署时，IP用域名代替，启动的时候可能需要先通过DNS域名解析得到IP，再连接，这会增加时间;(2).可能做的镜像有问题,打算用别人的image来测试，看是否会出现；

3.neonio支持在线部署的准备，申请dockerhub账号，上传镜像，ks meetup宣传材料准备提纲：（1）NeonIO 介绍；（2）NeonIO 部署；（3）NeonIO 展示；（4）NeonIO 案例 

4.支撑EQ环境zk/数据库迁移改造完成；


下周计划：
1.neonio ks meetup宣传材料准备；
2.neonio 故障和性能测试；




在云原生场景下构建企业级存储方案

neonio Quickstart：
1.检查确保有可给供neonio的设备；
2.检查是否已经安装QBD；
3.添加helm repo；
4.安装部署：helm install neonio ./neonio --namespace kube-system

单机部署：helm install neonio ./neonio  --namespace kube-system --set sc.rep_count=1 --set center.servers=1 --set zookeeper.servers=1 --set galera.servers=1 --set store.servers=1 --set web.servers=1 --set redis.servers=1
对接Promethus、altermanger: helm install neonio ./neonio  --namespace kube-system --set web.prometheus=true  --set web.alertmanager=true
指定节点部署存储服务：通过对对应节点打label
RDMA/TCP切换：helm install neonio ./neonio --namespace kube-system --set store.type=RDMA
物理硬件的充分利用：helm install neonio ./neonio --namespace kube-system --set store.port=eth0 --set rep_port.port=eth1
  
                             
rook-ceph Quickstart：
1.检查确保有可给供ceph的设备；
2.检查是否已经安装RBD；
3.apt-get install -y lvm2
4.下载代码：git clone --single-branch --branch master https://github.com/rook/rook.git
5.cd rook/cluster/examples/kubernetes/ceph
  kubectl create -f crds.yaml -f common.yaml -f operator.yaml
  kubectl create -f cluster.yaml
  
单机部署：cd rook/cluster/examples/kubernetes/ceph
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
kubectl create -f cluster-test.yaml
 
对接Promethus、altermanger: 
kubectl create -f service-monitor.yaml
kubectl create -f prometheus.yaml
kubectl create -f prometheus-service.yaml
指定节点部署存储服务：通过修改cluster.yaml配置，指定需要部署存储服务组件节点的IP
RDMA/TCP切换：ceph本身支持RDMA，rook-ceph不支持
物理硬件的充分利用：rook-ceph不支持
  
《在云原生场景下构建企业级存储方案》
随着云原生技术的日益普及的今天，对于无状态的应用已经非常成熟，有非常好的平滑扩展能力，但对于有状态的应用，数据需要持久化存储(即云原生存储)，这还有很大提升的空间，面临着很多挑战，本次分享主要有以下几点：
1.云原生存储面临的挑战？
2.目前云原生存储常见的解决方案，方案的不足、痛点；
3.面对这些挑战，不足、痛点，云原生存储NeonIO是如何面对的；
4.云原生存储NeonIO的性能表现；
5.云原生存储NeonIO的应用场景；



1.在云原生遇到的挑战章节最后加上CNCF的报告作为论点的支撑；
2.常用云原生存储解决方案中提下优势，不要只讲劣势；
3.部署灵活的那一页，字体太小，看不清；
4.高性能那一页数据有问题：应该是单卷iops 100K
5.最后一页内测计划，


本周工作：
1.neonio ks meetup PPT准备


下周计划：
1.neonio 故障和性能测试；

Random Read/Write IOPS: 19.4k/20.5k. BW: 778MiB/s / 882MiB/s
Average Latency (usec) Read/Write: 522.23/473.74
Sequential Read/Write: 803MiB/s / 850MiB/s
Mixed Random Read/Write IOPS: 13.5k/4491
root@testr01n01:~/dbench-test# kubectl logs dbench-single-mhxs5

Random Read/Write IOPS: 18.3k/19.1k. BW: 734MiB/s / 842MiB/s
Average Latency (usec) Read/Write: 511.06/513.35
Sequential Read/Write: 770MiB/s / 835MiB/s
Mixed Random Read/Write IOPS: 13.6k/4529

产品介绍/架构


本周工作：
1.出差上海，进行ks meetup 主题分享；
2.ks meetup PPT内容文章化，提供给ks团队；
3.研究helm charts和operate两种方式的部署；
4.研究怎么直接使用k8s的etcd服务；


下周计划：
1.继续研究neonio切换etcd


本周工作：
1.neonio的支撑：目前有几个反馈问题：
（1）.部署的时候zk store 没启动成功，没起来的原因都是他们环境资源不满足，应该都是一些爱好者，自己手里的环境都是在自己的电脑跑三个虚拟机来装的，所以这个资源上比较紧张，这些我通过给他们调整配置参数，勉强能够跑起来---这个我们可能需要在使用手册中增加部署neonio的规格说明；
（2）web pod起不来，需要手动删除这个pod，让k8重新启动一个--这个有大概的思路怎么解决；
（3）要不同OS版本的qbd包--这个跟晓军交流了一下，他们现在可以打出qbd dkms包，这个是qbd的kernel，可以适用在不同的OS版本，现在还需要把qbd的util部分容器化，这样的话neonio就不需要区分centos、ubuntu，以及不同OS的版本了

2.neonio的优化，web pod起不来，需要手动删除这个pod的问题；增加组件依赖的检查日志，方面遇到问题时的排查；

3.调研切换etcd的方式，目前倾向于单独部署etcd集群使用；

4.讨论交流qbd容器场景不在依赖OS的问题，有两种方案，目前使用在qbd做成静态方式，由晓军开发，直接提供neonio使用即可；


下周计划：
1.继续研究neonio切换etcd；
2.优化neonio；
3.接手公有云gd2a区运维交流；


本周工作：
1.修改neonio卸载后重新部署或者节点重启zk POD不能正常回复的问题；
2.neonio部署时自动能把满足条件的磁盘配置进store的，而无需指定磁盘的开发；
3.neonio的性能测试，测试neonio管理网口/数据网口的性能，两种场景配置方式性能没有区别，和neonsan的性能比较，没有性能损耗；
4.测试静态的qbd包，在ubuntu16.04.5 centos8.2作了测试，neonio是可以兼容centOS、ubuntu，不再需要区分ubuntu或者centos了；
5.neonio github上README.md资料的准备，已经准备地差不多了，还有一些格式的调整；
6.公有云gd2a区环境的熟悉；



下周计划：
1.继续研究neonio切换etcd；


https://git.internal.yunify.com/SAN2.0/qfa/wikis/gd2a-cluster-info

从7.240节点： san_gd2a 登录
或跳板机：ssh root@139.198.122.6 -p 4120 然后ssh 10.16.102.2 -p22

但是这种登录方式后面可能就不支持了，需要用何智上次说的那种方式


容器微服务小、快、轻、独立这种特性


本周工作：
1.完成neonio计算和存储分离部署的开发：即k8s集群1部署neonio，k8s集群2通过CSI使用k8s集群1的存储；
2.学习portworx云原生存储相关资料；
3.学习etcd V3 API接口是使用方式；




下周计划：
1.meetup材料准备；
2.值工单；

https://go.microsoft.com/fwlink/p/?LinkId=534599



HandleError->handleAioError-》increaseVolumeMetaVer
            handleNotPirmary-》increaseVolumeMetaVer
            handleSsdError-》increaseVolumeMetaVer
AsyncCheckSsdStatus->            handleSsdOffline-》increaseVolumeMetaVer
            handlePrepareFail-》increaseVolumeMetaVer
            
prepareVolume-> handleSsdAbsenseOnPrepare-》increaseVolumeMetaVer
RelocatePrimaryByStore-》increaseVolumeMetaVer
SetQos-》increaseVolumeMetaVer
DeleteQos-》increaseVolumeMetaVer
RepairVolume-》increaseVolumeMetaVer
DoRecoveryVolume-》increaseVolumeMetaVer
doReassignReplica-》increaseVolumeMetaVer
migrateReplicaById-》increaseVolumeMetaVer
DoScrubVolume-》increaseVolumeMetaVer
RelocatePrimary-》increaseVolumeMetaVer
RemoveSsd-》increaseVolumeMetaVer
CreateSnapshot-》increaseVolumeMetaVer
FailoverVolume-》increaseVolumeMetaVer
AddMetroReplica-》 increaseVolumeMetaVer
DelMetroReplica-》 increaseVolumeMetaVer
doSetVolumeRole-》increaseVolumeMetaVer
checkDiskErrorForVolume-》increaseVolumeMetaVer



center功能:
1.pool卷组相关的功能；
2.rg资源组相关的功能
3.volume相关的功能；
4.snapshot快照相关的功能
5.clone克隆相关的功能
6.故障恢复相关的功能，recovery volume/repair volume等


7.Qos相关的功能
8.故障恢复相关的功能，recovery volume/repair volume等
9.互斥卷组相关的功能
10.ISCSI管理相关的功能
11.灾备相关功能
12.加密相关的功能

NewJob MqInit

本周工作：
1.值工单：1.处理中金项目clone失败的问题；2.ARM环境中卷meta_ver递增到很大的问题；
2.梳理neonio与zk有交互的地方和center功能，考虑如果进行裁剪的化可以只保留哪些功能，学习client-go leader选举的代码；
3.lib库2.2.4 window的编译，编译已经通过，链接出现的问题，在和胡银坤正在分析；




下周计划：
1.neonio轻量化改造的事情。

本周工作：
1.neonio轻量化改造：（1）.研究client-go informer机制，学习使用Informer来进行监控/通知的实例代码；
                    （2）.client-go开发测试环境的配置安装，能够把demo代码编译通过并运行；
2.《NeonIO云原生存储应用及实践》博客内容的编写

下周计划：
1.继续neonio轻量化改造。



apt install -y librdmacm-dev libaio-dev libssl-dev uuid-dev libcurl4-openssl-dev


https://git.internal.yunify.com/SAN2.0/qfa/wikis/gd2a-cluster-info


从7.240节点： san_gd2a 登录
或跳板机：ssh root@139.198.122.6 -p 4120 然后ssh 10.16.102.2 -p22

但是这种登录方式后面可能就不支持了，需要用何智上次说的那种方式


云堡垒机
hansyang/Yangxing@198884

GD2A-广东2区 (GD2A) NEONSAN 设备调整网卡队列长度通知

内容：
	
尊敬的用户您好：
	
我们计划于2021年7月15日 22:00 - 2021年7月15日 24:00 对广东2区（GD2A）的NEONSAN 设备调整网卡的队列长度，正常情况下不会有任何影响。极个别极端情况下可能会 IO 长时间夯住(大于5分钟)， 则您需要将虚拟机关闭，然后重新开机即可解决。
我们的技术团队将在升级期间全程保障，感谢您长期以来对青云QingCloud的支持。

青云QingCloud

10.16.102.11
10.16.102.21
10.16.102.31


本周工作：
1.neonio轻量化改造：center leader 选举的修改，目前代码基本已经改完，待测试;
                    
2.运维：gd2a磁盘有media error告警定位处理；熟悉修改网卡队列文档和事先熟悉环境信息
    
下周计划：
1.继续neonio轻量化改造。





echo "lease_duration=2" >> /etc/neonsan/center.conf
echo "renew_deadline=2" >> /etc/neonsan/center.conf
echo "retry_period=1" >> /etc/neonsan/center.conf

retry_period < renew_deadline < lease_duration

root@gd2ar19n02:~# neonsan list_store
Store Count:  9
+----+--------------+------------+----------+--------+-----------+---------+-------------------+----------------+----------------+------------+
| ID |   MNGT IP    |    NAME    | POSITION | STATUS |  CHASSIS  | DOMAIN  | PHYSICAL CAPACITY | PHYSICAL USED  |  LOGICAL USED  | VOLUME CNT |
+----+--------------+------------+----------+--------+-----------+---------+-------------------+----------------+----------------+------------+
|  7 | 10.16.102.12 | gd2ar19n02 |        4 | OK     | ch1       | gd2ar19 |    32001801322496 |  1197004029952 |  6345814179840 |         19 |
|  8 | 10.16.102.22 | gd2ar20n02 |        1 | OK     | ch1       | gd2ar20 |    32001801322496 |  2069557673984 |  5712306503680 |         18 |
|  9 | 10.16.102.32 | gd2ar21n02 |        2 | OK     | ch1       | gd2ar21 |    32001801322496 |  1943216848896 |  3876207984640 |         15 |
| 10 | 10.16.102.10 | gd2ar19n00 |        3 | OK     | chassis19 | domain1 |    32006252888064 | 12931475439616 | 14901389033472 |         14 |
| 11 | 10.16.102.11 | gd2ar19n01 |        4 | OK     |         0 | domain1 |    23684627570688 | 12676528865280 | 11201274707968 |          9 |
| 20 | 10.16.102.20 | gd2ar20n00 |        1 | OK     | chassis20 | domain1 |    32006252888064 | 14191444361216 | 14845554458624 |         12 |
| 21 | 10.16.102.21 | gd2ar20n01 |        2 | OK     |         0 | domain1 |    23684627570688 | 12476410232832 | 11368778432512 |          7 |
| 30 | 10.16.102.30 | gd2ar21n00 |        3 | OK     | chassis21 | domain1 |    32006252888064 | 14860586844160 | 15092515078144 |         16 |
| 31 | 10.16.102.31 | gd2ar21n01 |        4 | OK     |         0 | domain1 |     7681501126656 |  4213161590784 |  3801046056960 |          3 |
+----+--------------+------------+----------+--------+-----------+---------+-------------------+----------------+----------------+------------+
hansyang@yunify.com
Yangxing@198884

openvpn --daemon --cd /etc/openvpn --config config.ovpn --log-append /var/log/openvpn.log

POD1="csi-neonsan-controller-ff598bbf5-txnpm"
POD2="csi-neonsan-node-q5fqd"
kubectl -n=kube-system exec $POD1 -- mkdir /etc/kubernetes/
kubectl -n=kube-system exec $POD2  -- mkdir /etc/kubernetes/
kubectl cp ../dockerfile/neonio-center/admin.conf -n=kube-system $POD1:/etc/kubernetes/
kubectl cp ../dockerfile/neonio-center/admin.conf -n=kube-system $POD2:/etc/kubernetes/

center异步任务的改造：
目前center利用zk实现如下14种工作队列的异步任务：
rebalance_cluster, stretch_transfer, recovery_volume, scrub_volume, snapshot_regular, metro_sync, rebalance_volume, rebalance_ssd, metro_sync_buffer, clone_volume_sync, gc, move_volume, general_operation, local_access
利用client-go代替zk实现工作队列异步任务，现在以recovery_volume的异步任务为例
1.在实现neonio的operate部署创建recovery_volume的CRD；
2.在center在anto_recovery开关打开检测到需要进行recovery_volume或者由cli触发recovery_volume的时候创建recovery_volume的CRD的对象；
3.center按照client-go中workqueue demo（https://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go）实现watch recovery_volume的CRD，当recovery_volume的CRD有add的操作（即创建recovery_volume的CRD的对象）会把recovery_volume的CRD的对象加入到queue中，由queue保证先进先出实现recovery_volume任务的执行顺序，任务执行成功更新recovery_volume的CRD的对象。

go get -v github.com/pelletier/go-toml
go get -v github.com/samuel/go-zookeeper
go get -v github.com/sirupsen/logrus
go get -v k8s.io/apimachinery
go get -v k8s.io/client-go
go get -v github.com/arthurkiller/rollingwriter
go get -v github.com/go-sql-driver/mysql
go get -v xorm.io/core
go get -v github.com/go-xorm/xorm
go get -v golang.org/x/sys
go get -v golang.org/x/crypto
go get -v github.com/go-uuid/uuid
go get -v github.com/robfig/cron
go get -v github.com/gogo/protobuf
go get -v github.com/google/gofuzz
go get -v xorm.io/builder
go get -v xorm.io/core
go get -v k8s.io/klog/v2
go get -v k8s.io/api
go get -v github.com/imdario/mergo
go get -v github.com/spf13/pflag
go get -v golang.org/x/term
go get -v golang.org/x/net
go get -v gopkg.in/inf.v0
go get -v github.com/google/go-cmp
go get -v sigs.k8s.io/structured-merge-diff/v4
go get -v github.com/golang/protobuf
go get -v github.com/googleapis/gnostic
go get -v golang.org/x/time
go get -v k8s.io/utils
go get -v github.com/go-logr/logr
go get -v github.com/davecgh/go-spew
go get -v golang.org/x/oauth2
go get -v github.com/json-iterator/go
go get -v gopkg.in/yaml.v2
go get -v google.golang.org/protobuf
go get -v github.com/modern-go/reflect2
go get -v sigs.k8s.io/yaml
go get -v github.com/modern-go/concurrent
go get -v golang.org/x/text
go get -v google.golang.org/appengine
go get -v k8s.io/client-go
go get -v github.com/mattn/go-runewidth
go get -v golang.org/x/crypto
go get -v github.com/go-sql-driver/mysql
go get -v golang.org/x/net
go get -v github.com/rivo/uniseg
go get -v github.com/satori/go.uuid
go get -v golang.org/x/crypto
go get -v golang.org/x/sys


kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph

docker load -i qbd-init.tar.gz 
docker load -i store.tar.gz 
docker load -i center.tar.gz


本周工作：
1.neonio轻量化改造：recovery_volume异步任务的用k8s workquque功能完成，基本功能测试通过，目前在进行genera_operation异步任务的修改，在整理代码框架，尽量代码复用，减少代码冗余度，
测试过程中发现的问题：
一进行recovery_volume出现leader center切换，原因是center与k8s的心跳超时，通过加长lease的占用时间，并且修改成部署的时候这个参数配置成可以配置；
k8s的lease锁机制进行leader选举，当所有center都挂后，k8s中lease的信息还存在，就会导致lib、cli等查询leader center ip不准确，可能要结合center的endpoint的功能来实现或者调研k8s kube-controller组件的选举机制

还有的工作:delete_job stop_job,list_job, cli触发job的时候可以指定job的执行时间；
                    
2.代码review：1296，1308

3.协助志阳搭建rook-ceph的环境；

4.SUSE-15-SP2编译环境的搭建，SUSE安装依赖包比较麻烦，跟晓军沟通优先级放低；

    
下周计划：
1.继续neonio轻量化改造；
2.值工单；

卷降级：
1.登录到leader center节点：grep -a "Set replica StatusError" /var/log/neoncenter.log
2.查询卷分布所在的节点： neonsan list_replica_location -- volume vol/xxxxxx
3.登录到卷分布的所在节点查store日志：grep -a "ReportError" /var/log/neonstore.log,查看时间前一两分钟报错情况

for client /127.0.0.1


2021-08-25 16:32:56,073 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@215] - Accepted socket connection from /192.168.101.28:39330
2021-08-25 16:32:56,073 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@903] - Connection request from old client /192.168.101.28:39330; will be dropped if server is in r-o mode
2021-08-25 16:32:56,073 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@949] - Client attempting to establish new session at /192.168.101.28:39330
2021-08-25 16:32:56,074 [myid:3] - INFO  [CommitProcessor:3:ZooKeeperServer@694] - Established session 0x3012a0927ffb795 with negotiated timeout 5000 for client /192.168.101.28:39330
2021-08-25 16:32:56,374 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x3012a0927ffb795, likely client has closed socket
2021-08-25 16:32:56,374 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1056] - Closed socket connection for client /192.168.101.28:39330 which had sessionid 0x3012a0927ffb795
2021-08-25 16:33:18,508 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@215] - Accepted socket connection from /192.168.101.25:40588
2021-08-25 16:33:18,508 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@903] - Connection request from old client /192.168.101.25:40588; will be dropped if server is in r-o mode
2021-08-25 16:33:18,508 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@949] - Client attempting to establish new session at /192.168.101.25:40588
2021-08-25 16:33:18,509 [myid:3] - INFO  [CommitProcessor:3:ZooKeeperServer@694] - Established session 0x3012a0927ffb796 with negotiated timeout 4000 for client /192.168.101.25:40588
2021-08-25 16:33:18,512 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1056] - Closed socket connection for client /192.168.101.25:40588 which had sessionid 0x3012a0927ffb796


grep -a "Accepted socket connection from /10.176.0.44" /var/log/zookeeper/


neonsan list_replica_location --store_id 30 >> list_replica_location_30.txt
neonsan list_replica_location --store_id 32 >> list_replica_location_32.txt

neonsan list_audit >> list_audit.txt


neonsan add_ini -initiatorip 172.16.11.161  -wwn iqn.1993-08.org.debian:01:592ba1712453
neonsan add_vol_ini -vip  172.16.11.181  -targetwwn iqn.2012-04.com.qs:san.test -initiatorip 172.16.11.161 -volume vol/test
iscsiadm -m discovery  -t sendtargets -p 172.16.11.181
lsblk
iscsiadm -m node -T iqn.2012-04.com.qs:san.test -l

fio --name=test --filename=D\:\test.bin --ioengine=windowsaio --thread=1 --direct=1 --size=10G --bs=1M --iodepth=1 --rw=read

fio --name=test --filename=C:\Users\yangxx\qingcloud\tmp\test.bin --ioengine=windowsaio --thread=1 --direct=1 --size=10G --bs=4k --iodepth=1 --rw=randwrite

win qbd:
2.32G = 2375.68MB
2分钟1s = 121S

2375.68/121 = 19.633MB/s 

D:\Windows => D:\QAX\Windows


ISCSI:
2.32G = 2375.68MB
50S
46S

2375.68/50 = 47.53

D:\Windows => D:\QAX\Windows




\\172.16.11.244\qax\Windows > \\172.16.11.244\新建文件夹\Windows 


kubectl edit role neonio-center -n neonio

- apiGroups:
  - neonio.qingstor.io
  resources:
  - generaljobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete 
- apiGroups:
  - neonio.qingstor.io
  resources:
  - recoveryjobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete 
- apiGroups:
  - neonio.qingstor.io
  resources:
  - gcjobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - neonio.qingstor.io
  resources:
  - movejobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - neonio.qingstor.io
  resources:
  - clonejobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - neonio.qingstor.io
  resources:
  - snapshotjobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete 
- apiGroups:
  - neonio.qingstor.io
  resources:
  - rebalancessdjobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - neonio.qingstor.io
  resources:
  - rebalanceclusterjobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - neonio.qingstor.io
  resources:
  - scrubvolumejobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - events.k8s.io
  resources:
  - Event
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
  
  
gd2a 3台neonsan闪存节点的测试：

1）检查网口bond方式为同网卡的2个口做bond --配置正确

2）新全闪RG应该有SSD,SSD2这两个LABEL。

3）如果是对老RG进行扩容，则要求1（以后默认bond方式为同网卡做bond），而不要求2，并且老RG必须不能有SSD2这个LABEL，以防止E3主机用RDMA连接老RG 存储节点。


neonsan create_volume --volume test/test_e3 --size 1000G --repcount 2 --rg testSSD2

fio -direct=1 -filename=/dev/qbd0 -name=test -iodepth=32 -bs=4k  -numjobs=1 -rw=randwrite -ioengine=libaio -time_based -runtime=60 -group_reporting

neonfio -direct=1 -volume=test/test_e3 -name=fio_test_4k_randwrite_32_1  -ioengine=qbd -bs=4k -runtime=60 -size=128G -time_based -rw=randwrite -config_file=/etc/neonsan/qbd.conf  -use_tcp=0 -group_reporting -iodepth=32 -numjobs=1

neonfio -direct=1 -volume=test/test_e3 -name=test -ioengine=qbd -bs=4k -runtime=60 -size=128G -config_file=/etc/neonsan/qbd.conf -rw=randwrite -time_based -use_tcp=1 -group_reporting -iodepth=32 -numjobs=1

neonfio -name=case -ioengine=qbd -direct=1 -bs=4k -volume=test/test_e3  -config_file=/etc/neonsan/qbd.conf -rw=randwrite -iodepth=32 -time_based -numjobs=1 -runtime=60 -use_tcp=0

qbd -m rdma://test/test_e3

fio -direct=1 -filename=/dev/nvme3n1 -name=test -iodepth=64 -bs=128K  -numjobs=1 -rw=write -ioengine=libaio -time_based -runtime=60 -group_reporting

root@gd2ar25n06:~# fio -direct=1 -filename=/dev/nvme3n1 -name=test -iodepth=64 -bs=128K  -numjobs=1 -rw=write -ioengine=libaio -time_based -runtime=60 -group_reporting
test: (g=0): rw=write, bs=128K-128K/128K-128K/128K-128K, ioengine=libaio, iodepth=64
fio-2.2.10
Starting 1 process
Jobs: 1 (f=1): [W(1)] [100.0% done] [0KB/2695MB/0KB /s] [0/21.6K/0 iops] [eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=6756: Tue Sep 28 12:20:47 2021
  write: io=164436MB, bw=2740.5MB/s, iops=21923, runt= 60003msec
    slat (usec): min=5, max=1038, avg=15.05, stdev= 4.55
    clat (usec): min=1111, max=9993, avg=2903.03, stdev=530.73
     lat (usec): min=1127, max=10004, avg=2918.26, stdev=530.77
    clat percentiles (usec):
     |  1.00th=[ 2128],  5.00th=[ 2256], 10.00th=[ 2352], 20.00th=[ 2480],
     | 30.00th=[ 2672], 40.00th=[ 2800], 50.00th=[ 2896], 60.00th=[ 2960],
     | 70.00th=[ 3056], 80.00th=[ 3152], 90.00th=[ 3376], 95.00th=[ 3728],
     | 99.00th=[ 4448], 99.50th=[ 5280], 99.90th=[ 8640], 99.95th=[ 9536],
     | 99.99th=[ 9792]
    bw (MB  /s): min= 2600, max= 2831, per=100.00%, avg=2741.13, stdev=49.94
    lat (msec) : 2=0.22%, 4=97.32%, 10=2.46%
  cpu          : usr=22.52%, sys=26.34%, ctx=1203752, majf=0, minf=10372
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued    : total=r=0/w=1315485/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: io=164436MB, aggrb=2740.5MB/s, minb=2740.5MB/s, maxb=2740.5MB/s, mint=60003msec, maxt=60003msec

Disk stats (read/write):
  nvme3n1: ios=41/1313948, merge=0/0, ticks=8/3802884, in_queue=3750776, util=99.85%
  
root@gd2ar25n06:~# fio -direct=1 -filename=/dev/nvme3n1 -name=test -iodepth=64 -bs=128K  -numjobs=1 -rw=read -ioengine=libaio -time_based -runtime=60 -group_reporting
test: (g=0): rw=read, bs=128K-128K/128K-128K/128K-128K, ioengine=libaio, iodepth=64
fio-2.2.10
Starting 1 process
Jobs: 1 (f=1): [R(1)] [100.0% done] [3227MB/0KB/0KB /s] [25.9K/0/0 iops] [eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7158: Tue Sep 28 12:22:33 2021
  read : io=193007MB, bw=3216.7MB/s, iops=25733, runt= 60002msec
    slat (usec): min=4, max=608, avg=12.88, stdev= 3.67
    clat (usec): min=67, max=33698, avg=2472.82, stdev=1172.06
     lat (usec): min=81, max=33709, avg=2485.91, stdev=1172.05
    clat percentiles (usec):
     |  1.00th=[  548],  5.00th=[  868], 10.00th=[ 1112], 20.00th=[ 1432],
     | 30.00th=[ 1784], 40.00th=[ 2192], 50.00th=[ 2416], 60.00th=[ 2544],
     | 70.00th=[ 2800], 80.00th=[ 3312], 90.00th=[ 4048], 95.00th=[ 4704],
     | 99.00th=[ 5728], 99.50th=[ 5920], 99.90th=[ 7328], 99.95th=[ 9280],
     | 99.99th=[15424]
    bw (MB  /s): min= 2172, max= 3257, per=100.00%, avg=3216.86, stdev=98.54
    lat (usec) : 100=0.01%, 250=0.02%, 500=0.58%, 750=2.94%, 1000=3.53%
    lat (msec) : 2=28.00%, 4=54.52%, 10=10.35%, 20=0.04%, 50=0.01%
  cpu          : usr=6.87%, sys=43.25%, ctx=858444, majf=0, minf=8273
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued    : total=r=1544052/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: io=193007MB, aggrb=3216.7MB/s, minb=3216.7MB/s, maxb=3216.7MB/s, mint=60002msec, maxt=60002msec

Disk stats (read/write):
  nvme3n1: ios=1542417/0, merge=0/0, ticks=3793708/0, in_queue=3751192, util=99.86%

dd if=/dev/zero of=/dev/nvme3n1 bs=10M count=2048 oflag=direct

DELETE FROM candidate_disk WHERE id=47；



本周工作：
1.完成gd2a e3的性能测试:SSD本身性能及网络损耗顺序读写未达标（iodepth=32，bs=64k，顺序写2207MB，读2694MB，写2669MB，标准顺序读2999）
裸SSD测试数据 bs=128K，iodepth=64，顺序读写3216.7/2740.5（官方性能3450/2850）),扩容的e3服务器已经按照要求加入新建的SSDRG2资源组
                    
2.neonio完成rebalance_ssd,rebalance_cluster,scrub_volume异步任务的改造，目前异步任务的改造已经完成；

3.完成UOS、麒麟ARM NeonSAN的版本的编译打包，已经提供给neil;

curl "http://192.168.101.174:2600/qfa?op=enter_maintenance&store_id=192168101174"
curl "http://192.168.101.174:2600/fast_chan?op=quit_maintenance&store_id=192168101174"
curl "http://192.168.101.174:2600/qfa?op=stat_replica"
curl "http://192.168.101.174:2600/qfa?op=trim_ssd&ssd_uuid=7bb65eeb-fbfb-45e0-9b4b-02473c34f23f"
curl "http:192.168.101.174:2600/qfa?op=rebalance_primary_replica&limit=1"

议题：QingStor NeonIO云原生与存储之实践
QingStor NeonIO是一款支持容器化部署的企业级分布式块存储系统，能够给Kubernetes平台上提供动态创建（dynamic provisioning)持久存储卷（persistent volume)的能力,支持clone、snapshot、resstore、resize等功能。
本次分享将系统介绍QingStor NeonIO的云原生架构和应用实践，以及在应用实践过程中遇到的问题和解决办法。

内容大纲：
1.QingStor NeonIO云原生系统架构介绍；
2.QingStor NeonIO实际应用案例；
3.使用云原生架构带来交付、运维上的效率提升；
4.云原生与存储未来演进趋势思考；

听众受益：
1.了解云原生与存储最新的演进趋势
2.了解QingStor NeonIO云原生与存储的实践经验；
3.了解云原生化后如何带来开发、交付、运维效率上的提升；



本周工作：
1.neonio轻量化去zk改造完成：
2.研究openebs CAS容器存储，研究jiva存储的引擎的使用和原理；



议题：从QingStor NeonIO谈云原生存储架构的演进和思考
QingStor NeonIO是一款支持容器化部署的企业级分布式块存储系统，能够给Kubernetes平台上提供动态创建（dynamic provisioning)持久存储卷（persistent volume)的能力,支持clone、snapshot、resstore、resize等功能。
本次分享将介绍云原生存储的挑战、目前的常见的解决方案、NeonIO的云原生架构以及对未来架构的演进和思考。

内容大纲：
1.什么是云原生存储；
2.云原生存储面临的挑战；
3.CAS容器存储的理念；
4.常见云原生存储的解决方案；
5.介绍NeonIO架构和原理；
6.云原生存储架构的演进方向和思考；

听众受益：
1.了解云原生与存储最新的演进趋势
2.了解QingStor NeonIO云原生存储的架构和原理；
3.了解云原生化后如何带来开发、交付、运维效率上的提升；
4.了解云原生存储后续架构的演进方向；

痛点：
1.大规模动态快速的创建销毁PV的能力；
2.抛弃传统分布式的思维方式，拥抱云原生CAS架构；


主题：云原生存储介绍和应用实践
大纲：
1.什么是云原生存储
2.为什么需要云原生存储
3.云原生存储两种路线
4.QingStor的云原生存储及应用实践

问题1：上面提到OpenEBS、Rook-Ceph存产品的对比，同时也提到他们的优点点和缺点，请问一下，neonio相比于他们，有什么优势？
答：在性能上的优势，从上面提到的性能表现中，neonio在IOPS、时延上都有很大的优势，NeonIO采用全闪的分布式存储架构，支持RDMA，client端采用自研的qbd协议（当然也支持普通磁盘HDD，TCP网络连接），天生就是为了高IPOS、低
时延而生，同时也支持clone、snapshot、restore、resize等企业级特性，OpenEBS LocalPV没有高可用功能，cstor、jiva性能比较差，当然既说openEBS正在开发的存储引擎mayastor专为解决性能问题设计，支持nvme磁盘，客户端协议用nvmeof，我们也在期待，而ceph的IO路径很长，性能很难和neonio相比。

问题2：我们在使用需要过程中，需要用到跨主机的pod使用共享目录,neonio支持文件存储吗？
答：不支持,但是我们qingstor的其他团队后面也会推出云原生文件存储


curl "http://$(qfcip):2600/qfa?op=diff_snapshot&volume_name=test10&pool_name=vol&base=snap1&end=snap2"

在云原生技术日益普及的今天，云原生技术在生产环境中越来越多的被使用，用户应用正在从无状态应用程序扩展到需要持久化存储支持的有状态应用程序

store节点上线：
1：OnClientConnStatusChange-》UpdateStoreStatus（更新到数据库） -》 HandleConnByStore（发送给store建立连接）
2. WatchStoreTimer（从数据库查询store状态，如果是offine） -》 HandleConnByStore（发送给store断开连接）


https://golang.google.cn/dl/go1.16.windows-amd64.msi



我觉得我们用vscode阅读代码和编写代码的时候不能跳转，阅读代码很不方便，需要很多一些复杂的配置才能实现代码跳转，我觉得这是一个痛点，有同事也说过不能跳转非常的难受，因此我总结了一下vscode配置，并且进行了一些自动化，现在配置起来简单多了，有兴趣的或者有需要的可以按照如下链接进行配置：https://cwiki.yunify.com/pages/viewpage.action?pageId=89791729




1.安装go编译环境
# 安装go
tar -zxvf go1.12.5.linux-amd64.tar.gz
mv go /usr/local/
export PATH=/usr/local/go/bin/:$PATH

# 安装gopath
tar -zxvf  gopath1.12.5.tar.gz
mkdir -p /z0
mv z0/gopath /z0

2.解压代码，执行编译脚本
tar -zxvf neonsan_jenkins_v3.1.9.tar.gz
cd neonsan_jenkins_v3.1.9/builder/qfa/scripts/
./jenkins.sh


1.跟踪上周工单遗留下来蓝月亮的问题：虚拟机重启原因是磁盘故障导致dispatcher夯，导致center卡住，引起openvolume超时的问题，目前服务找硬件厂商找故障原因，没有找到原因，现在是直接换了盘；
2.gd2a cas缓存盘更换完成；
3.整理了一下neonio除了去mysql外目前还可以做的一些工作，并初步排了一下优先级；
4.学习备份功能和IO流程的代码，反馈到vscode看代码不跳转的问题，于是就把之前的配置梳理一遍，并自动化，形成wiki文档；



 dpkg -r pitrix-neoncenter-dbg pitrix-neonsan-tool pitrix-neonstore-dbg && cd /z0/yangxingxiang/qa/debs-ubuntu_16.04.5 && dpkg -i pitrix-neoncenter-dbg-3.2.1.0.0.1.deb pitrix-neonsan-tool-3.2.1.0.0.1.deb pitrix-neonstore-dbg-3.2.1.0.0.1.deb && supervisorctl stop neoncenter neonstore && supervisorctl start neoncenter neonstore

cd /z0/yangxingxiang/qa/debs-ubuntu_16.04.5/ && dpkg -i pitrix-neoncenter-dbg-3.1.1.0.0.1.deb pitrix-neonsan-monitor-3.1.1.0.0.1.deb  pitrix-neonsan-tool-3.1.1.0.0.1.deb pitrix-neonstore-dbg-3.1.1.0.0.1.deb && supervisorctl stop neoncenter neonmonitor neonstore && supervisorctl start neoncenter neonmonitor neonstore

#export PROXY="http://192.168.101.231:8990"
#export PROXYS="https://192.168.101.231:8990"
#export http_proxy=$PROXY
#export https_proxy=$PROXYS
#export HTTP_PROXY=$PROXY
#export HTTPS_PROXY=$PROXYS
