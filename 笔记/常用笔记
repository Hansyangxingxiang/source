hansyang@yunify.com 
QkKkPjNV0P 

OA：https://track.yunify.com/secure/RapidBoard.jspa?rapidView=73&quickFilter=166&quickFilter=150
https://cwiki.yunify.com/login.action?os_destination=%2Findex.action&permissionViolation=true
yangxx/Yangxing@1234

https://yunify.anybox.qingcloud.com/folders/13146
https://yunify.anybox.qingcloud.com/folders/742099
这个是安装部署文档

mysql -uneonsan -pzhu88jie -h192.168.0.2  "source /etc/neonsan/initdb.sql"

手动搭建neonsan环境遇到问题记录：
1.ubuntu启动zookeeper报错：Syntax error: "(" unexpected (expecting "fi")
原因：ubuntu_16.04.5 /bin/sh软连接到dash，有可能是对应的zookeeper的版本支持ubuntu_16.04.5不太好
解决办法：把/bin/sh 软连接到bash。

2.安装galera数据库时按照指导文档配置好源后，无法安装成功，报源中找不到对应的安装？
解决办法：用dpkg -i 安装，这个比较麻烦，无法解决安装包依赖问题，需要把依赖的包一个一个的安装，比较麻烦。

3.安装neonsan后，发现center、stor、monitor一直在重启？并且收到内存过高的告警？
原因：刚开始环境内存不够，把stor的参数改小，依然在不断的重启，后面通过排查发现center、monitor配置中配置的mysql数据库密码没改对。
通过搭建环境对neonsan的理解：neonsan组件包含：zk集群、Mysql数据库集群、center控制集群、stor存储节点、monitor等组成。
zk集群：统一管理控制节点和存储节点；
Mysql数据库：存储集群相关元数据；
center控制集群：卷创建、删除等的控制；
stor存储节点：负责数据io请求：
monitor：监控模块。


monitor模块学习：：
1.由neonmonitor, prometheus, alertmanager组成，prometheus, alertmanager为业界开源的组件，实际neonmonitor是实现Prometheus的Exporters，把metrics暴露给prometheus；
2.monitor阅读代码理解，监控分三个大的部分：
（1）monitorStore： store iops，store tcp network，store rdma network，Cas Core Write，io usage，port status
（2）monitorStatus：store status，store ssd，volume status，center role，mysql-plus status，zookeeper status
（3）本节点：node cpu percen，node memory
3.monitor获取数据的方式：调用neonsan的api 接口获取；在本节点执行go cmd 或调用go的工具包获取。

NeonSAN 编译学习：访问 VCenter 控制台虚拟机创建；

1.创建卷报错：INFO[0000] create volume failed. Reason:HTTP status:500  rc:-1 reason:Failed to choose volume place: resource group nodes not enough for create volume
原因：创建卷需要创建资源组；

2.创建资源组时报：FATA[0000] Failed to add resource group node, reason:HTTP status:200  rc:-507 reason:store id 2 not exist.
原因：搭建环境时有两个节点的stor id忘记改，并且重复了，重新修改配置，重启服务，重启不不生效，需要手动更改数据库。
为什么重启不能生效，需要清除数据库？
重新清除数据库：drop database neonsan;source /etc/neonsan/initdb.sql;各个节点centor,store
为什么kill -9 store，store重启，centor会重启？

创建一个3副本的卷，其中有两个节点重启，导致2个副本error，store重启后为什么不能自动恢复？需要neonsan set_parameter --parameter auto_recovery --value 1
为啥不能开关默认开启？

3.日志文件中无法看到打印日志的是在那个文件的哪一行？

副本粒度64G


下载代码：
git clone -b qa https://git.internal.yunify.com/SAN2.0/qfa.git
git clone -b qa https://git.internal.yunify.com/SAN2.0/qfcenter.git
git clone -b qa https://git.internal.yunify.com/SAN2.0/monitor.git
cd qfa/
git submodule init 
git submodule update


创建Job流程：
1."local_access"的任务，如果任务属于WAITING，PENDING，如果卷名相同，需要把已经存在的任务删除；
2.如果存在完全相同未完成的任务，则报错;
3.如果METRO_SYNC，且任务已经完成或者失败，则需要把任务删除；
4.如果是waiting jobs，waitingJobs不能大于等于队列queue.Capacity的一半；
5.waitingJobs + pendingJobs的任务不能大于等于queue.Capacity；
6.waitingJobs + pendingJobs的任务不能大于等于queue.Capacity；
7.创建任务；
8.如果不是waiting的任务，则把waiting的任务变成pending的任务；

删除任务：
1.带jobId：job是pending或者processing的任务不能删除；从zk中删除，然后从队列中删除；
2.删除一段时间期间的job:循环遍历任务，只删除在时间期间内不是PROCESSING、PENDING的任务；

任务执行：
启动：
MqInit
1.创建集群队列节点，初始化队列MAP；
2.初始化队列消费函数；
3.从队列中获取所有队列名；
4.在个队列节点下创建WAITING, PENDING, PROCESSING, COMPLETED, FAILED的子节点；
5.把PROCESSING的job已到FAILED；
6.设置队列capacity
7.内存中保存queueInfo，并且把zk的job信息保存到内存；
8.zk中创建各个队列；
9.job任务执行;executeTimingJobs 定时把waiting的任务移到pending；doExecute：把pending的job移到processing，并执行任务函数；
10.clearOverdueJob清除超时任务。

统信UOS和华为泰山ARM平台的NeonSan的编译打包：
1.实际就是在新的架构平台上实现NeonSan的编译打包，由于新的架构平台NeonSan编译依赖的lib库没有现存的（比如zk、toml），在编译NeonSan之前需要用源码编译对应的lib库；
2.libtoml：一种可以解析配置文件的库，有点类似于解析json格式的配置文件的模块库；、
3.在新的平台中编译可能会编译错误，需要修改适配对应的源码；


本周工作：
	1.完成NEONSAN-623：neonsan add_ssd超时问题；
	2.修改NEONSAN-634：升级脚本检查qingstor neonsan-mysql状态检查代码编写，过程中梳理升级脚本流程并输出文档流程，文档如附件，已经进行脚本的单函数测试，整套升级脚本流程待搭环境测试，整体完成 %80；
	3.完成ubuntu18.04.01 arm OS的neonsan的编译打包；
	4.学习nesonmonitor代码，prometheus框架；nesonmonitor实际为prometheus的一个exporter组件，nesonmonitor 利用Prometheus client API注册收集指标；在本地完成Prometheus，node_exporter、graph的环境搭建；学习node_exporter已经有哪些资源监控信息；
	

下周计划：
	1.解决NEONSAN-34：正在recovery的卷无法停止；
	2.搭建环境验证升级流程；
	3.学习版本发布流程和代码学习；
	4.ubuntu18.04.01 arm OS的neonsan的编译打包流程脚本自动化；




升级流程：
1.基本工具检查：basic_tools_check
（1）检查当前节点是否安装jq 、curl、nohup工具；
（2）集群的所有节点必须安装prlimit、ping、nc工具；
2.检查集群状态：check_cluster_status
（1）检查store状态；check_store_status curl发rest消息获取所有store的IP，curl发rest消息回去获取store的状态，检查是否OK；     问题：获取store IP是从数据库中获取，还需要执行neonsan list_store的目的？
（2）检查ssd状态： check_ssd_status
（3）检查port状态：check_port_status，获取所有port端口的IP，检查每个IP端口的状态；
（4）检查各服务状态：check_supervisor_service_status，ssh到每个节点执行supervisor命令查询；supervisorctl status；neoncenter neonstore neonmonitor zookeeper，检查数据库的状态
（5）检查check_started_live_nodes
（6）检查卷的状态：check_volume_status
（7）检查job状态；pending|processing|waiting状态的job退出
3.设置升级的原版本和目标版本：set_version；
4.拷贝升级到目标节点：copy_upgrade_packet；
5.升级前准备：preparation_before_upgrade：备份数据库，查询卷、ssd、store、port、parameter信息，重定向到本地文件；把auto_balance、auto_recovery开关关闭；
6.升级到center：upgrade_center_service：先升级leader，检查集群状态，检查内容和步骤2一样，备份配置center配置文件，覆盖安装软件包，删除supervisor的center服务，重新reload，检查升级后的版本，等待center被拉起，检查center的状态    问题：fellower的所有节点为啥不能批量升级？
7.检查升级后的版本和目标版本是否一样：check_db_ver
8.升级tool：upgrade_tool_service：获取集群的节点，一个一个节点的升级，备份qbd.conf配置，移除too的软件包，然后安装，检查升级的版本是否正确；
9.升级store：upgrade_store_service：升级前状态检查，检查是否存在ERROR的卷，存在错误或者降级的卷，则设置自动恢复卷的功能，检查集群的状态，执行升级store的操作：检查资源组：没有资源组会创建资源组，进入维护模式，备份配置，覆盖安装软件包，等到store状态OK，退出维护模式，检查升级后的版本是否符合预期，升级后检查各卷的状态，进程服务状态；
10.升级monitor：upgrade_monitor_service：备份配置，覆盖安装软件包，配置没有，则添加mysql_cluster，这个分成galer数据库和xenondb，修改timeInterval的值，添加local_storeip，重启monitor服务，检查版本
11.升级数据服务：upgrade_galera_cluster_service：检查数据状态，备份配置，覆盖安装，恢复配置，重启服务，检查版本，检查状态；
12.升级后的操作，查询资源组、卷、ssd等信息，升级完成后的清理；
13.结束；
14.退出维护模式；

wget https://studygolang.com/dl/golang/go1.12.5.linux-arm64.tar.gz
tar -zxvf go1.12.5.linux-arm64.tar.gz  -C /home/yangxx/
tar -zxvf gopath1.12.5.tar.gz -C /home/yangxx/
export GOROOT=/home/yangxx/go
export GOPATH=/home/yangxx/gopath
export PATH=$GOROOT/bin:$PATH
git clone -b v2.5.0-release https://git.internal.yunify.com/SAN2.0/qfa.git
git checkout -b v2.5.0-release
git clone -b v2.5.0-release https://git.internal.yunify.com/SAN2.0/qfcenter.git
git clone -b v2.5.0-release https://git.internal.yunify.com/SAN2.0/monitor.git
git clone https://git.internal.yunify.com/SAN2.0/libtoml.git

yangxx@ubuntu18041-compile:~/libtoml$ make
make[2]: *** No rule to make target 'RAGEL-NOTFOUND', needed by 'toml_parse.c'.  Stop.
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/toml.dir/all' failed
make[1]: *** [CMakeFiles/toml.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 

apt install ragel

[ 80%] Building C object CMakeFiles/toml.dir/toml_parse.c.o
toml_parse.rl:13:10: fatal error: unicode/ustring.h: No such file or directory
 #include <unicode/ustring.h>
          ^~~~~~~~~~~~~~~~~~~
compilation terminated.

改动有这些文件：
modified: store/src/core/store_node.go
modified: store/src/main.go
modified: store/src/qfa_md5.cpp
modified: thirdparty/zookeeper/src/c/Makefile.am
modified: thirdparty/zookeeper/src/c/acinclude.m4
modified thirdparty/zookeeper/src/c/configure.ac

apt-get install libicu-dev
apt-get install uuid-dev
apt-get install libaio-dev


NEONSAN-653	
[云平台易捷版]galera 集群在断电/恢复电力之后不能自动恢复
1.1.2版本自愈流程：
1.bootstrap有1，就是首节点:（bootstrap是最后关机，数据是最新）
2.如果找不到bootstrap为1的节点，就找seqNO最大的节点当首节点；（seqNO大数据最新）
3.如果seqNO都相同，那就随便找一个当做首节点，目前找IP地址大的当首节点。

现在模拟断电/恢复：三个节点同时kill -9 mysqld进程多次，发现无法自愈：
不能自愈原因：有一个节点获取seqNO失败，无法选出首节点

目前运维经验中遇到无法获取的seqno的解决方法是：获取seqno命令中加--user=mysql
所以当前要做的是：
1.看能否从以前遇到问题的局点中去到日志，或者后面遇到这种问题收集日志分析，确认不能自愈也是获取的seqno命令缺少--user=mysql；
2.如果是，自愈流程获取seqno的命令中也加上--user=mysql。
3.当前遇到加--user=mysql也无法获取seqno，网上解决方法是直接删除ib_logfile0，ib_logfile1，分析这种操作的风险，为后面运维积累经验，或者如果没有风险，这个步骤也做到自愈流程中。


 NEONSAN-34 【recovery volume】正在recovery的卷无法停止：1.删除一个正在恢复的卷，提示正在恢复；2.增加停止recovery的CLI
 
 neonfio -name=case -ioengine=qbd -direct=1 -bs=4k -volume=pool_name/yangxx4  -config_file=/etc/neonsan/qbd.conf -rw=randwrite -iodepth=10 -time_based -numjobs=1 -runtime=500 -use_tcp=1

 本周工作：
	1.完成NEONSAN-653[云平台易捷版]galera 集群在断电/恢复电力之后不能自动恢复：修改、验证：已经push，待review；
	2.完成NEONSAN-34 【v2.1.5】【recovery volume】正在recovery的卷无法停止：修改、验证：已经push，待review
	3.完成NEONSAN-665 neonsan stop_job优化，已经合入3.0；
	4.出ubuntu 18.04.1 v2.5.1 arm版本的包。
	
下周计划：
	1.熟悉了解《大规模集群方案》；
	
	
galera_cluster 数据库初始化未同步，一段时间内数据不一致的问题；
https://galeracluster.com/library/documentation/schema-upgrades.html

kill -9 mysql进程，查看集群状态

从10节点登录firstbox 172.31.45.2
172.31.45.10 express1ar01n00
172.31.45.11 express1ar01n01
172.31.45.12 express1ar01n02 

sync;date;echo 1 > /proc/sys/kernel/sysrq && echo b > /proc/sysrq-trigger



export BRANCH_NAME="qa"
export BUILD_PATH=/z0/yangxingxiang/qa
export BUILD_VERSION="debs-ubuntu_16.04.5"
export BUILD_ONLY="true"

当使用新版本qbd时： Store 与client 保持原来的zk 配置不变，升级结束后，老qbd通过qbd.conf 中的配置依然与当前rg 的local center 建立连接， 并且将zk ip list 和local leader center写入到一个本地文件中，下次open_volume时 直接从本地文件中连接zk 获取local leader center ; 若失败则连接上次保存的local leader center。
当使用的qbd版本不支持保存local leader center 和local zk 到本地文件时，老的qbd 每次都从qbd.conf 中获取qfcip 与local center进行连接。
当扩容了一个新rg 之后，老版本的qbd.conf 要访问新rg 上的卷时， 在通过qbd.conf 连接到local center 后， local center 检查卷不属于本资源组， 于是转发到global center , global再转发到对应的rg 进行处理。在这种情况下， 新版本的qbd将只在首次需要进行两次转发， 后续都可以从本地文件获取新的所属rg local center。 老版本的center 则每次都需要转发2次。

https://tenant.quanxiangyun.com/qingcloud/manage/tickets/1598
富春云prometheus的iops信息为0。检查集群状态正常，节点的node-exporter，获取curl 9100/metrics信息正常，prometheus都正常：
原因：统计iops的变量类型（sig_atomic_t）是32位的，运行时间长了可能会越界出现负数，monitor上报prometheus的时候出现这种异常时直接上报为0，并且monitor中直接把lastIoStat清理，没有把本次的结果赋值给lastIoStat，导致一直上报0.
修改代码：把本次的查询结果赋值给lastIoStat，出现负数的时候，属于增量计算的，也能正常吧结果上报到prometheus
临时的解决方法：依次重启rg下面节点的store进程（重启进程影响较大，谨慎操作）

https://boss.qingcloud.com/tickets/tk-mqtsiogds3l#
泉州移动neonsan混插环境，iops只有1.2k左右，在交付前请研发检查。tv1 403 307 099.   w8rn69
分析：之前看错了，iops 12k左右，没有预热的情况下是正常，按照最后一步测试，大致可以测出来30K左右的iops，符合验收标准，需要按照交付文档测试

https://tenant.quanxiangyun.com/qingcloud/manage/tickets/1627
青岛创恒    私有云  sanc 环境  neoncenter 剩一个节点 running，现在启动了三台节点的neoncenter，neonsan命令还是报错
FATA[0002] Failed to list ssd. reason:HTTP status:<nil>, reason:Get http://10.21.0.21:2600/qfa?op=list_ssd&refresh_capacity=false: dial tcp 10.21.0.21:2600: connect: connection refused 
虚拟机无法启动
客户着急，麻烦尽快上来看下

tv 1350 470 643 2cwg57
center的一个历史bug，在切换center时偶尔可能会阻塞，客户用的还是老版本
root@neonsan41:~# neonsan list_version
NeonSAN Cli Version: 2.2.0 build:82399ce-Jan 10 2020 01:21:36
+---------+-------------------------------------+---------------------+
| NEONSAN |            BUILD VERSION            |    UPDATED TIME     |
+---------+-------------------------------------+---------------------+
| center  | 2.2.0,build:359eb7e-20200110.012237 | 2020-01-16 22:25:24 |
| monitor | 2.2.0,build:67e0050-20200110.012237 | 2020-01-16 23:14:28 |
| store   | 2.2.0,build:82399ce-20200110.012237 | 2020-01-16 22:35:47 |
+---------+-------------------------------------+---------------------+

https://track.yunify.com/browse/NEONSAN-163
https://track.yunify.com/browse/NEONSAN-363

https://boss.qingcloud.com/tickets/tk-hukrs78s2qa#
用户虚机挂载的vos盘 目录有段时间无法写入，排查系统日志，有这个报错，这个报错的触发原因需要协助排查一下

问题原因：网络异常引起center切换，当前版本存在一个bug，job队列超了会引起center切换失败，建议升级到新版彻底解决。

https://boss.qingcloud.com/tickets/tk-hukrs78s2qa# 338 137 162  1qaz@WSX   teamviewer
用户虚机挂载的vos盘 目录有段时间无法写入，上周五也有同类问题，期间持续了将近一个小时

root@neonsan41:~# cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-08-25 16:23:42.222] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-08-25 17:08:42.099] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-08-28 17:39:02.951] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 09:55:45.765] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan41:~# ssh 10.21.0.11
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-39-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
New release '18.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Thu Sep 10 13:38:19 2020 from 10.21.0.2
root@neonsan11:~#  cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-04 23:57:00.439] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:44:39.874] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:47:18.261] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:04:57.815] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:18:34.303] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:19:45.681] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:21:46.671] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:26:01.882] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan11:~# ssh 10.21.0.31
Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-39-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
New release '18.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Wed Sep  9 09:54:37 2020 from 10.21.0.31
root@neonsan31:~#  cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-05 00:23:34.631] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 09:49:35.607] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
root@neonsan31:~# exit
logout
Connection to 10.21.0.31 closed.
root@neonsan11:~# ll /var/log/* -h^C
root@neonsan11:~# cat /var/log/neoncenter.log |grep "failed queue"
INFO[2020-09-04 23:57:00.439] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:44:39.874] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-08 23:47:18.261] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:04:57.815] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:18:34.303] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:19:45.681] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:21:46.671] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275
INFO[2020-09-09 00:26:01.882] Moving processing queue's job into failed queue... message_queue.go qfc.MqInit 275


https://boss.qingcloud.com/tickets/tk-fprwho2yprd#
泰康保险 私有云 neonsan服务器重启问题排查
检查过系统日志看不出由于软件导致的重启。需要驻场同事装一下impitool 收集硬件信息


https://boss.qingcloud.com/tickets/tk-3njr9cn4bc4# 上海科创测试neonsan环境，客户反馈在使用iscsi时，有时候会在3点和23点左右连接iscsi中断。检查kern日志io 有delay，继而检查center日志，发现当时在删除snapshot，但是都删除失败了，发现只有19和29这2台删除失败，到19和29检查store日志发现有持续的SLOW IO告警，iostat看磁盘没有到瓶颈，负载也不高，麻烦帮忙看下原因


https://boss.qingcloud.com/tickets/tk-dobjwzaim99
私有云  上海科创
21 22节点neonstore服务异常退出，麻烦看下
tv12        1 421 688 504           isy584

科创这个问题在2.5.2解决了，在store节点连接其他store的时候，用select的方式设置了连接的超时时间，当select监听的套接字fd大于1024时，就会造成函数栈帧内的内存溢出，所以退出函数的时候crash。已经fix掉


本周工作：
1.完成升级脚本jira单NEONSAN-689、NEONSAN-691 修改验证，代码已经merge，升级脚本v1.12已经发布归档；
2.完成NEONSAN-669、NEONSAN-670、NEONSAN671、NEONSAN-672修改，代码已经push，待review；
3.工单处理和跟踪，目前本周有5个工单，4个已经有结果，1个还在分析中；
4.发布v2.5.2的版本；
5.解决Jenkins打包第三方库没有更新到最新节点的问题；

下周计划：
1.大规模集群方案优化中lib common的适配修改；
2.修改jira单；

http://old-releases.ubuntu.com/releases/16.04.5/


cd /root/debs-ubuntu_16.04.5/ && dpkg -r pitrix-neoncenter-dbg pitrix-neonsan-monitor pitrix-neonsan-tool pitrix-neonstore-dbg && dpkg -i pitrix-neoncenter-dbg-3.0.0.0.0.1.deb pitrix-neonsan-monitor-3.0.0.0.0.1.deb  pitrix-neonsan-tool-3.0.0.0.0.1.deb pitrix-neonstore-dbg-3.0.0.0.0.1.deb && supervisorctl stop neoncenter neonmonitor neonstore && supervisorctl start neoncenter neonmonitor neonstore


本周工作：
1.分析青岛创恒 center切换为主之后卡住，引起center切换的原因是center连接不上zk，分析zk异常的原因，可能是存在的bug；https://issues.apache.org/jira/browse/ZOOKEEPER-1582
2.完成NEONSAN-715变量越界，iops显示为0的修改和验证，代码已经merge；
3.完成NEONSAN-701 API list_volume支持互斥组名过滤的修改验证，代码已经push，待review；
4.完成NEONSAN-113 API list_store 支持rg_id rg_name,rg_label过滤；
5.完成NEONSAN-264 list_replica_location 支持更多查询选项，代码已经push，待review；
6.完成lib common中open_volume_common流程中获取qfcip流程适配修改的测试，此修改方法可能对一个节点对接多套neonsan（即一个节点多个qbd.conf）的场景不支持；


下周计划：
1.修改jira单；

ubuntu iso下载地址：
http://old-releases.ubuntu.com/releases/16.04.5/


open_volume_comcom的查询qfcip流程适配修改：
原来流程：通过/etc/neonsan/qbd.conf获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询得到qfcip，
修改成如下：
1. /etc/neonsan/local_zk_qfc.conf存在，且local_zk ip local cluster_name 和qbd.conf相同：则用/etc/neonsan/local_zk_qfc.conf查询，否则用/etc/neonsan/qbd.conf查询
（1）.从 /etc/neonsan/local_zk_qfc.conf 获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询（流程和原先的不变）
（2）.如果（1）查询失败，从/etc/neonsan/qbd.conf查询（查询流程和1一样）
（3）.如果是通过/etc/neonsan/qbd.conf查询成功的，把zk ips和查询到的qfc IP保存到/etc/neonsan/local_zk_qfc.conf中


本周工作：
1.NEONSAN-684分析，3.0.0已经不会在zk 里面写uuid，问题不存在；2.5.x版本中一般remove ssd应该也不会出现zk存在、数据库中不存在的情况，除非从zk中删除的时候异常，没有return，然后又把数据库中的删除，因此解决此问题需要从源头解决，即，从zk删除异常时返回失败，不继续从删数据库删除。
2.open_volume_comcom的查询qfcip流程适配修改和验证，可能和最初的需求理解有偏差，还需进一步分析设计；
3.完成ubun18.04版本包的自动化；
4.neonsan容器化部署研究，初步想了一些镜像打包的思路和方法，下一步还需要参考业界分布式存储容器化的方案、容器team团队给的赋能和经验分享；
5.v3.0.1、v2.5.5的打包工作，并且出手动出v3.0.1使用openssl库的打包；

下周工作：
1.继续neonsan容器化部署研究；
2.open_volume_comcom适配的分析、设计、修改；


本周工作：
1.完成NEONSAN-544修改验证，代码已经push，待review；
2.完成NEONSAN-764修改验证，代码已经push，待review：
3.完成NEONSAN-593修改验证，代码merge
4.完成NEONSAN-801修改验证，代码merge；
5.完成NEONSAN-766修改验证，代码已经push，待review：
6.完成v2.5.5版本发布相关材料准备和归档，v3.0.1打包相关工作。

下周工作：
1.继续neonsan容器化部署研究；
2.open_volume_comcom适配的需求澄清，修改、验证；


open_volume_comcom的查询qfcip流程适配：
背景：后续的大规模部署新架构将会是一个global center 管理多个local center , 每个local center 集群负责它所管理的rg的卷的错误处理和open volume等操作，这次需求的适配为了兼容后续超算项目从3.x 版本升级到新架构的场景。期望当使用新版本qbd时，Store 与client 保持原来的zk 配置不变
原来流程：通过/etc/neonsan/qbd.conf获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询得到qfcip，
修改成如下：
1. /etc/neonsan/local_zk_qfc.conf存在，且local_zk ip local cluster_name 和qbd.conf相同：则用/etc/neonsan/local_zk_qfc.conf查询，否则用/etc/neonsan/qbd.conf查询
（1）.从 /etc/neonsan/local_zk_qfc.conf 获取zk ips从zk 查询qfcip，查询失败从上次缓存在内存中qfcip查询，再失败从/etc/neonsan/qbd.conf center.ip 遍历查询（流程和原先的不变）
（2）.如果（1）查询失败，从/etc/neonsan/qbd.conf查询（查询流程和1一样）
（3）.如果是通过/etc/neonsan/qbd.conf查询成功的，把zk ips和查询到的qfc IP保存到/etc/neonsan/local_zk_qfc.conf中




#define MAX_BUFFER_SIZE_128  128
#define MAX_BUFFER_SIZE_1024  1024


struct qfa_options {
    char userkey_name[MAX_BUFFER_SIZE_128];
    char userkey_passwd[MAX_BUFFER_SIZE_128];
    char zk_ips[MAX_BUFFER_SIZE_1024]
    char center_ips[MAX_BUFFER_SIZE_1024];
}

int open_volume_common(struct qfa_client_volume *vol, int lib_ver, struct qfa_options *options);


lib common适配修改：
1.options->zk_ips, options->center_ips没有内容时，open_volume_common按照原流程查询qfcip；
2.options->zk_ips, options->center_ips有内容时，open_volume_common使用options->zk_ips, options->center_ips查询qfcip；
3.查询到qfcip，向leader center发送open_volume（此步骤不涉及改动）；
4.解析open_volume返回的卷的属性，把所属的zk_ips,center_ip保存到options->zk_ips, options->center_ips中；

center修改：
1.OpenVolume API输出中增加参数zk_ips，center_ips；
2.两参数直接从该节点的/etc/neonsan/center.conf中读取zookeeper.ip,center.mngt_ip；

qbd修改：从缓存读取到内容（读取不到赋值空）后调用open_volume_common，调用open_volume_common完后从options->zk_ips, options->center_ips读取内容进行缓存；



2020年10月23日讨论纪要问题：
1.qemu qfa_open_volume的缓存问题；
2.qbd client老版本不支持时，open_volumem每次都需要global转发，即：qbd-> global -> local， local-> global->qbd；
3.center_ips缓存多个，leader center ip放在第一个；

引起同步时间太长导致会话超时的因素，1、网络，2、磁盘io，3、ticktime
网络都是不可靠的，不能通过方面去解决，只能通过磁盘，ticktime可以考虑调整，从切换center时间中去取舍
另外我现在想在本地中看能不能复现出来



本周工作：
1.完成open_volume_comcom适配的需求分析，澄清，代码开发完成，待测试，总体完成60%；
2.分析浦发硅谷银行zk不稳定导致临时节点消失问题：zookeerper 服务在同步日志过程中耗时太长，花了9465ms，网上查找相关案例，同步日志ZK无法响应外部请求，即无法检测到心跳，进而引发session过期，session过期会引发zk临时节点的消失，根据日志查看zk源码，代码中只是简单的把事务日志刷入磁盘，不涉及到节点之间的的网络交互，却耗时9000多ms，因此在本地环境复现，在zk的节点，给系统盘增加io压力，但是没有出现fsync-ing同步超时的告警，于是通过修改zk代码复现，leader 节点出现fsync这个会出现session超期，follower节点出现fsync概率性出现follow er与leader连接断开。

下周工作：
1.继续neonsan容器化部署研究；
2.继续open_volume_comcom适配的需求、开发验证；
3.3.0.1版本打包、准备发布材料等相关工作；






本周工作：
1.完成open_volume_comcom适配的自验证，qbd进行适配后在整体测试下，总体完成90%；
2.完成NEONSAN-813开发验证，代码已经merge；
3.完成NEONSAN-836 NeonSAN自动化打包时生成每个包的md5值开发，代码已经merge；
4.主要考虑后续neonsan容器化部署时zk、mysql数据需要持久化，学习k8s容器持久化存储中PV PVC StorageClass相关技术特效；
5.完成3.0.1版本发布相关材料准备归档，3.0.2打包，v2.5.5,V3.0.1手动生成每个包的md5值，已经归档到anybox；

下周工作：
1.bug 840的分析和修改;
2.继续neonsan容器化部署研究；

本周工作：
1.分析浦发硅谷银行导致磁盘IO压力的原因，补充了一些可能引起磁盘io压力过大的原因和完善故障报告；
2.完成NEONSAN-840开发验证，代码已经push，review有一些检视意见，待修改；

下周工作：
1.qbd open_volume_comcom的验证;
2.neonsan容器化部署研究；


SELECT s.uuid, s.free, t.threshold FROM ssd s, (select CEILING(AVG(free)*0.9) AS threshold FROM ssd, store WHERE store.status='OK' AND ssd.status='OK' AND ssd.store_id=store.id AND store.rg_id IN (SELECT store.rg_id FROM store, ssd WHERE ssd.store_id=store.id AND ssd.uuid='09f55ec5-a359-4390-af1b-f21c549fe533')) t  WHERE s.uuid='09f55ec5-a359-4390-af1b-f21c549fe533';


1)肯定的是升级过程不会冲掉个性化配置。zookeeper 的问题在升级之前即存在，只是没有引发NeonSAN故障。
2)我们在排查时结合zookeeper的源代码和日志，发现故障的直接原因是写IO超时，这个环境是融合部署环境，物理节点上既有zookeeper，NeonSAN存储服务，NeonSAN监控，又有虚拟机在运行，其中zookeeper,NeonSAN 日志，以及监控都会在线往系统盘写日志，因此会存在相互影响的情况。通过对比EQ的环境，EQ的环境负载CPU、内存资源使用率比较小，日志没有出现过fsync-ing同步时间超长的告警，而GD环境负载一般都是70%以上，日志中经常出现fsync-ing同步时间超长的告警。
3)zookeeper的最佳实践也建议将zookeeper作为独立集群进行单独部署，当前的融合部署模式不符合zookeeper的最佳实践要求。官网中关于磁盘性能对zookeeper写事务的影响和dataLogDir的配置建议说明：http://svn.apache.org/repos/asf/zookeeper/trunk/docs/zookeeperAdmin.html：

这个我想这样改：
rebalanceSsd函数中循环调用policy.ChooseAltReplicaPlace查询，首次调用的时候第三个参数restrict.ExcludeStoreList空，查询的replicaPlace经过后面检查如果不满足，把replicaPlace对应的storeId加到restrict.ExcludeStoreList再次查询，直到查询满足条件的或者查询失败的结束，查到满足条件的可以迁移，失败则不能迁移



neonsan create_volume --volume poo/vol0  --size 2T  --repcount 2 --thick_prov 

1.构造集群中如下场景：
domain1 store 31 43 5个ssd 剩余大概800G,  800G平摊到5块ssd中
domain2 store 41    2个ssd 剩余大概700G， 700G平摊到2块ssd中,
domain3 store 42    2个ssd 剩余大概100G， 100G平摊到2块ssd中,

2.选择domain3 store 42一块使用率较高的ssd，rebalance_ssd，此时 domain store ssd的排序是：
domain1 43 ssd1
domain1 31 ssd2

domain2 41 ssd1


现在会先尝试迁移到domain1 43 ssd1，domain1 31 ssd1,不满足，最后迁移到domain2 41 ssd1


neonsan remove_ssd --ssd_uuid 6f99432e-42d8-4fd0-ba1e-003d38f4e2bd  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid e9c84436-c130-47b0-bb3a-48fe458fbed6  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 2dfbb532-b24c-4fbb-8262-42975485d6ab  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 9ed4c7d5-7d01-4f8d-9fec-6d1d9bf607c9  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 10ada5e3-bfa3-44d1-9556-cd9f039d2abc  --evacuate_data no
sleep 10

neonsan remove_ssd --ssd_uuid c9701ff0-36c0-422c-b7af-838bcb3522f5  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 45e0d44e-287f-49cc-a06f-1438fcad06e9  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid 3fe15523-c355-4426-ab2d-f2f5136d0a53  --evacuate_data no
sleep 10
neonsan remove_ssd --ssd_uuid f4e41ae4-8f7d-473c-82d1-a2701d4c7d52  --evacuate_data no


neonsan rebalance_ssd --ssd_uuid f29fa52d-6d26-4b9f-8d89-72fff948fa48


本周工作：
1.qbd适配完成后，大规模部署open_volume_common的整体测试，没有发现什么问题，代码已经push，待review：
2.完成NEONSAN-840场景未考虑全的问题；
3.Neonsan需要支持centos8.2，centos8.2版本的打包，总体已经完成%60；


下周工作：
1.centos8.2版本的打包、相关驱动下载和验证；

git clone https://github.com/Open-CAS/open-cas-linux.git
cd open-cas-linux
git checkout origin/v20.3.3
git submodule update --init
./configure
make
make install

本周工作：2020-11-23 -2020-11-29
1.工单处理：
    1.私有云华创：一块已经删除的硬盘为降级状态告警  --告警残留，按照 https://cwiki.yunify.com/pages/viewpage.action?pageId=23670454处理，第二天向付裕确认，问题解决；
    2.私有云富春云：客户自己搭建的一台redis主机，在今天上午10:42:39发生了主从切换 --查看Neonsan相关日志，无相关异常日志，应该不是neonsan存储问题引起；
    3.私有云滕州市政府：早上有部分卷降级触发了qbd error告警--当时磁盘io压力大，aio_post和aio_comp的个数不一样，可能就是盘没处理完，会引起卷降级；--看下日志是否有COW
    4.川庆砖探：这个环境又断电后 卷降级了  recovry失败--rose帮看，recovery失败时因为磁盘空间不足引起，解决措施：gc了一下，现在打开auto_balance，先均衡，然后recovery成功；
    客户通过iscsi连不上neonsan环境--改了SCST的配置，，客户连上了卷
    5.泉峰集团一个环境有slow io 大量的告警 -- 按照https://cwiki.yunify.com/display/NeonSAN/05-SLOW+IO发现是save_metadata引起，何智已经修改配置参数优化，暂时没有出现告警；-补充作何改动，现在环境是否保留改动？
    6.英大人寿，有2台虚拟机开机失败，iaas这边报 qbd 挂载超时 30s
    1.看syslog日志open_volume_common[vol/vosi-d7u0x9we.img:/etc/neonsan/qbd.conf]:28 错误码2；
    2.查看当时center日志，发现center日志也收到open volume的消息，但是后面没有响应的日志了；
    3.查看center日志数据库盘满了导致的，导致请求都在执行InsertAuditRecord的位置卡住了，所有的请求都没有得到处理。

    改进措施：
    1.清理的大的日志文件alertmanager.log；
    2.在logrorate中配置alertmanager.log日志切割转存；防止再出现空间爆满--已经实施
    7.英大人寿问题2：用户给数据盘做个备份，center日志中报空间不足--排查发现是alertmanager.log日志过大空间占满，解决错误清理日志，配置上logrotate日志切割转存；
    8.泰康私有云:老版本的neonsan （1.0.9） 一个 dw 管理节点（只有zookeeper和mysql） 根文件系统只读了，需要重启，能帮忙检查一下，是否可以直接重启?-- 检查两个节点的zk和myql的是正常的，这个节点可以直接重启，硬盘和raid 卡都检查过，没有发现问题
    9.sh1a公有云：vos-95w11bvd没有创建成功--NeonSAN收到的，和反馈失败的卷名字不一样，通过NeonSAN测试了一下创建卷，正常的；
     是哪个用户创建的？
     usr-gFS5392T，通过备份创建mysql，然后卡在创建硬盘了
     可能是还在导数据？
     感觉是盘太大，job超时了，可以加大 创建的 超时时间
     snapshot恢复成vos花了 15 小时；
     从snapshot节点 获取数据解压 并导入neonsan 。 一个管道进行。 没有限速。如果要优化的话，我觉得需要从neonsan入手。 分析neonsan import_diff 是否可以快
     do_create_volume_from_snapshot 这个函数import_diff的输入时export_diff出来的么？
     我们研究一下，看能否加速，在混插环境的 import_diff？
     qbd从2.1.0版本支持挂载volume snapshot，如果比较好的多线程读写工具，也可以换种思路导出。import_diff 如果能实现肯定是最快的
     应该是可以实现的
     备份如果只用快照实现，有什么问题吗？备份链的话，性能影响大不大？友商基本都是快照，不想我们要导到外部；
     现在还不支持，需要后续开发
     备份基于快照实现，其实备份的数据还是在当前的存储设备上；
     怎么能登录执行 import_diff的节点呢？先分析一下性能慢的原因；
     硬盘创建好了，主机状态有问题，麻烦看下
     sh1a 巡检发现交控有个plus集群异常了 neonsan有做调整吗？报的是文件系统的错误
     这是neonsan volume 吗
     是的
     没有做过
     mysql 无论如何不可能造成文件系统故障的，iaas 也不会
     通过qemu后端挂载的neonsan volume， qemu日志里没有报错
     这个待会得找一台，先做snapshot，然后用xfs_repair修复，但三台都坏，非常蹊跷
     vm内部也没有io error，第一次报错时间基本一致，
        Nov 27 19:07:46 i-m7voa7sn
        Nov 27 19:07:45 i-n5dh1g5b
        Nov 27 19:07:45 i-0wxq90b5
    我刚刚联系上客户了  这个是基于他们的集群备份创建的新集群
    那意思就是通过备份创建的盘是坏的
    是的
    客户允许我们重启 我先重启一下
    那跟下午的问题是一回事吧
    是的
    重启了也一直报那个错；
    得repair，xfs error不会自己就好了；
    这个有备份 可以直接修复；
    先打个快照
    客户是基于快照来创建的集群 已经有备份了
    我这边操作不了 从vm里 没办法umount
    得把mysql停掉，lsof /data没有占用了再试试
    mysql都没有启动，之前看mysql的日志是这样子的
    lsof /data可以看到，我在操作i-0wxq90b5，得从db应用层面停，杀掉之后还会起来
    kill掉几个进程以后 可以umount，我在修复i-m7voa7sn
    i-0wxq90b5:修完挂载不报了
    我操作下
    可以了，你重启下集群再看下还报错么
    好的，有问题，mysql起不来
    说明备份恢复过程里面有数据异常
    @Ada 如果客户基于备份重新创建一个集群 能解决吗？db那边不好修复 而且数据没了
    原集群还在吗
    在的
    原集群的卷是哪个？
    我的意思是这个备份很可能就是坏的。
    cl-vo5fqdzh，源集群是这个；
    那这个集群还是好的吗？
    是的，我明天联系客户 让客户重新创建一个备份，然后试下
    为什么非要通过备份创建，不是有克隆吗？
    mysql plus没有克隆，只能基于备份创建新的
    导入的过程中打了很多snapshot，不过应该是每次导入一个备份之后就删掉了，后续可以把这些snapshot留着，如果最后一个启动不了，可以rollback试一下
    明天做备份得做全量备份，再创建？
    vol/vos-hrdg2z6s_ss-gkvu9ko8#vos-95w11bvd_.img 这个卷名的意思是 vos-95w11bvd 是卷vos-hrdg2z6s的备份卷？
    不是，z6s是原盘，ko8是快照ID，在用这个快照生成最终卷 bvd；
    嗯，明白了，看到备份过来的卷数据跟原卷大小有些差别，那可能是这个snapshot之后原卷又有写入；
    肯定有写入，明天看全量备份有没有异常吧
    好的
    交控根据备份创建新集群失败了，今天根据备份: ss-opoydyll重新创建了一次集群，还是失败，麻烦看下
    还是失败指的是什么失败呢，文件系统不可用吗？
    创建集群job失败了，所以运维账号没创建
    那你得找到集群job失败的原因；
    到原因了，是因为这个节点重放redo比较慢，用时6分，其他节点用时2分钟，由于该节点没有重放redo、迟迟没有加入集群，因此这个节点restore时没走完全部流程，其实带来的影响主要是运维账号没初始化，我们较新版本是可以自动订正的，即使restore指令失败了，集群状态也会自动订正。db同事已经帮客户把这个集群修复好了
    
    10.泉州移动私有云# 迁移数据时，一个volume 降级：原因跟480应该是一样的，本质上都是之前的open volume hung住了，在volume set新meta version之后这个open volume请求才到达store，store上面新的meta version比这个openvolume里的新，所以报这个错，480在2.5版本里面已经fix了
    11.客户前端业务使用sanc，后端oracle rac  使用独立neonsan全闪存储， 客户反馈这周访问oracle比较慢。能否麻烦帮忙一起检查下 咱们2个存储池的性能，确定没问题，让客户找应用方再排查下业务 ：https://tenant.quanxiangyun.com/qingcloud/manage/tickets/2430
    检查网络没有延迟，日志没有slow io，何智帮忙检查：确认了是 SQL 的问题，跟Neonsan没关系。
    
    
    
    
     

2.Neosan支持centos8.2：
    1.编译脚本，rpm包的编译整合成一套，代码已经push;
    2.SCST驱动编译打包，已经编译成功；
    

下周任务：
    1.Bug修改和验证；
    2.neonsan容器化部署研究；
    



周四上午10：00准备review一下bitmap的最新设计文档以及代码，大家时间有没有冲突的？


设计文档：https://yunify.anybox.qingcloud.com/f/3682955
https://git.internal.yunify.com/SAN2.0/qfcenter/commit/dde9c4dcac08d481bfd40e4ecc6d3bba9f12f556
https://git.internal.yunify.com/SAN2.0/qfa/commit/c15cd87d111d47570cd0a5a118d0fd7e89331840

09-31204-750025-997324


systemctl stop firewalld
systemctl disable firewalld
/usr/sbin/sestatus -v
getenforce
sed -i 's/enforcing/disabled/' /etc/selinux/config
setenforce 0
/usr/sbin/sestatus -v
swapoff -a 
sed -ri 's/.*swap.*/#&/' /etc/fstab

cat >> /etc/hosts << EOF
192.168.101.2 k8s-master
192.168.101.3 k8s-node1
192.168.101.4 k8s-node2
EOF

yum install ntpdate -y

cd /root/debs-ubuntu_16.04.5/ && dpkg -r pitrix-neoncenter-dbg pitrix-neonsan-monitor pitrix-neonsan-tool pitrix-neonstore-dbg && dpkg -i pitrix-neoncenter-dbg-3.0.2.0.0.1.deb pitrix-neonsan-monitor-3.0.2.0.0.1.deb  pitrix-neonsan-tool-3.0.2.0.0.1.deb pitrix-neonstore-dbg-3.0.2.0.0.1.deb && supervisorctl stop neoncenter neonmonitor neonstore && supervisorctl start neoncenter neonmonitor neonstore


本周任务：
1.v3.0.2版本打包、归档、相关材料的准备；
2.neonsan支持centos8.2代码入库后，在jenkins自动化出包的测试，发现一个脚本无执行权限的问题已经解决；
3.补充open_volume_common适配的测试场景；
4.熟悉kubsphere容器平台，熟悉在其平台上部署mysql、创建任务等操作。

    

下周任务：
    1.Bug修改和验证；
    2.继续熟悉使用kubsphere容器平台；
    
    
最近涉及到KA客户以及公有云的环境改造和优化项目大约有下面几项，需要大家跟一下，把每个项目分配到个人了，后面有其他的支持再轮流安排。 注意，这些具体实施由服务团队同事负责，我们要帮助他们确认好方案，以及解决实施过程中遇到的技术问题。
富春云混插环境的优化调试（方案已经提供，需要跟客户讨论）素香
浦发硅谷环境的整体改造（已经草拟方案，需要研发参加评估）兴祥（远志支持）
泰康老版本存储的升级（离线还是在线方案还在等待客户确认）国武，佳宁
泰康需要2位研发保持工作日和非工作时间手机畅通 国武，佳宁
公有云RDMA适配 马强

admin/Zhu88jie

本周工作：
1.大规模部署open_volume_common适配检视意见的修改；
2.熟悉kubsphere容器平台：
（1）在其平台上创建企业空间、项目、帐户和角色；
（2）在其平台上创建存储卷；
（3）在其平台上部署自制的应用，应用下面添加服务组件，服务组件分成有状态的服务和无状态的服务；Neonsan的容器化部署，我的理解实际上就是部署一个Neonsan的应用，neonsan组件包括：zk、mysql、center、store、monitore，其中zk、mysql是用状态的服务，即需要持久化存储一些数据。
3.硅谷环境的整体改造方案分析和zk在线扩容和减容的验证；



下周工作：
    1.Bug修改和验证；
    2.继续熟悉使用kubsphere容器平台；
    
    
本周工作：
1.Neonsan容器化部署：
（1）制作Ubuntu1604基础镜像；
（2）基于Ubuntu1604基础镜像制作zookeeper的镜像；
（3）基于Ubuntu1604基础镜像制作galera mysql的镜像；
（4）基于Ubuntu1604基础镜像制作center的镜像；
（5）基于Ubuntu1604基础镜像制作store的镜像；
（6）基于Ubuntu1604基础镜像制作monitor的镜像；       (7)验证制作出来的镜像容器化部署：目前容器化的网络网络模式host的，网络与物理机使用相同的网络空间，网络不隔离。store的容器需要把磁盘直通到容器里面，目前除了monitor不能单独容器部署（原因是monitore依赖store的配置。并且monitor需要通过ssh远程查询zk和mysql的状态），其他功能正常。

2.V3.0.2版本重新打包归档；



下周工作：
    1.验证浦发硅谷应该数据库导出后导入到新库后数据的完整性；
    2.继续容器化部署研究；
    

本周工作：
1.Neonsan容器化部署与容器团队的石永红交流讨论，其解答了一些疑问和概念上的问题，并给了一些接接下来容器化部署的步骤：
  （1）把容器的配置写成一些yaml文件，用kubectl apply -f yaml的方式把neonsan部署和启动；
  （2）把这些yaml打包成helm chart，把neonsan做出可以放到应用商店的应用；
2.完成ubuntu20.04 neonsan安装包的自动化；
3.完成v3.0.3的bug NEONSAN-917修改和验证；
4.浦发硅谷数据库导出数据的完整行检查，首次导出的数据没有包含neonsan相关的函数和过程，加-R参数再次导出已经包含
  


下周工作：
    1.继续容器化部署研究；



试用期总结1.工作内容及工作成果 ：
1.参与neonsan开发和维护：（1）开发支持stop evict_store、rebalance_ssd、recovery_volume的功能特性，完成30+的bug的修复；（2）完成大规模部署open_volume_common的适配开发；
2.负责neonsan的编译打包和支持新的OS版本、以及neonsan版本的发布。（1）新增支持ubuntu18.04.5、centos8.2、ubuntu20.04 OS的neonsan版本，并且已经实现自动化；（2）优化编译脚本，编译脚本统一，使开发人员都能使用本地脚本编译打包版本，极大提高项目团队的工作效率；（3）负责V2.5.0-V2.5.4、V3.0.0-V3.0.2 9个版本相关材料的准备和发布；
3.负责galera数据库的开发和维护：（1）解决了galera_cluster集群全部宕机，出现缺少/data/galera/mysql/mysql无法自愈、galera_cluster集群宕机两个节点，节点恢复后集群无法自愈等4个BUG；
4.负责zookeeper相关问题定位和分析：（1）分析定位浦发硅谷银行临时节点消失引起故障的原因，并提出相应的解决优化措施；（2）青岛创恒 center切换为主之后卡住，引起center切换的原因是center连接不上zk，分析zk异常的原因；（3）整理和验证zookeeper扩容和缩容的方案；（4）整理和验证Xenondb数据库迁移到galera数据库的方案；
5.工单处理：处理工单15+个；
6.负责neonsan容器化部署：目前已经完成各个组件的镜像制作、镜像已经在docker中部署验证通过；

2.下一步的工作计划：
 1.把容器化部署的配置写成一些yaml文件，用kubectl apply -f yaml的方式把neonsan部署和启动；
 2.把这些yaml打包成helm chart，把neonsan做出可以放到应用商店的应用；
 3.参与neonsan开发和维护，更加熟悉neonsan功能特性和优势，比如极短IO路径，为neonsan设计和开发更有竞争力的功能特性，为neonsan商业化更加成功贡献自己的一份力量
试用期总结填写提示试用期总结内容必须包含以下两项：1.工作内容及工作成果 2.下一步的工作计划
对公司的建议1.公司代码规范化，制定C、 C++、go等语言的编程规范，所有相关的研发部门都遵守同一种规范，利于代码的开发和维护；
2.加强CICD持续集成、持续交付流水的建设，比如所有研发部门使用同一套流水线平台，各个部门的CICD经验可以得到分享，从而提高开发效率；
3.部门内加强各种技术经验的分享和交流活动，比如每两周技术经验分享或者编程技巧、算法的分享，加强工程师文件建设。


现在升级脚本问题：如果节点较多，没升一个节点需要输入节点IP，容易输错或者忘记升级到哪个节点，有可能会重复升级或者漏升级节点

修改方案1：能够一键式完成升级，正常的话不需要多次输入节点IP
1.升级center时，获取到center的IP_LIST,然后遍历IP LIST按照现在流程升级，如果是leader center，跳过，待其他follower节点升级完最后再升级center leader
2.每升级成功一个节点，记录下升级成功的节点，如果过程中升级异常，退出，排查解决后，继续按照步骤1升级，然后遍历IP LIST判断是否已经升级，已经升级的直接跳过；
3.store的也是同样的方式


保守一点的修改方案2：
升级流程和1一样，只是升级加入一个交互式的人为确认（y/n）是否继续升级下一个节点，可以人为检查升级过程是否有异常打印，也不需要每次输入节点IP。

NEONSAN-939问题引入回溯：
问题原因：
1.最初count返回的满足过滤条件的总数，701的需求加上支持group_name过滤后，因为对pn count的理解不完全，返回把pn count也当做一种过滤后卷的数量；
2.修改完成后和portal联调不充分；


修改：count返回除了pn、count，满足所有过滤条件的卷的数量；


本周工作：
1.Neonsan容器化部署与容器团队的石永红交流讨论，其解答了一些疑问和概念上的问题，并给了一些接接下来容器化部署的步骤：
  （1）把容器的配置写成一些yaml文件，用kubectl apply -f yaml的方式把neonsan部署和启动；
  （2）把这些yaml打包成helm chart，把neonsan做出可以放到应用商店的应用；
2.完成ubuntu20.04 neonsan安装包的自动化；
3.完成v3.0.3的bug NEONSAN-917修改和验证；
4.浦发硅谷数据库导出数据的完整行检查，首次导出的数据没有包含neonsan相关的函数和过程，加-R参数再次导出已经包含
  


下周工作：
    1.继续容器化部署研究；
    
    

本周工作：
1.Neonsan容器化部署与容器：
（1）熟悉了解rook的架构：rook包含自身的两个pod：rook-discover,rook-ceph-agent，其他存储自身的pod；
（2）了解kubernets yaml的格式，尝试把zk的部署写成yaml配置，用kubectl apply -f yaml部署，遇到不使用本地镜像制作的镜像而是pull远端镜像的问题，在石永红帮助下已经解决；
  
2.浦发硅谷迁移改造：已经梳理出zk迁移、数据迁移的详细步骤，并且与黄蔚然沟通细节，整个项目实际的操作细节黄蔚然在元旦节整理出来，然后组织review；
3.完成bug NEONSAN-939修改和验证，安装包已经更新到portal 周杨环境，待其确认；
4.完成升级脚本每升级一个store需要输入节点IP问题的优化；代码已经push，待review；
  

下周工作：
    1.继续容器化部署研究；
    2.继续跟进浦发硅谷迁移和组织文档review；
    
    

本周工作：
1.浦发硅谷迁移改造方案文档编写、测试验证；
2.解决centos8.2 neonsan安装包无法解决的问题；

下周计划：
1.继续容器化部署研究；
2.浦发硅谷支撑；

export PROXY="http://192.168.101.231:8990"
export PROXYS="https://192.168.101.231:8990"
export http_proxy=$PROXY
export https_proxy=$PROXYS
export HTTP_PROXY=$PROXY
export HTTPS_PROXY=$PROXYS

mount -t nfs 192.168.101.10:/z0 /z0



本周工作：
1；NEONSAN-943容器化部署NeonSAN:完成zookeeper的部署，正在进行galera数据库的部署，总体进度完成50%；

下周计划：
1.继续容器化部署；
2.浦发硅谷迁移支撑；

2020年年度工作总结：
1.专业技能、岗位核心能力

 